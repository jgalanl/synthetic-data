{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.single_table import CopulaGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 50\n",
    "NUM_EPOCHS = 5_000\n",
    "NUM_SYNT_DATA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_half(x):\n",
    "    return round(x * 2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Definir reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_constraint = {\n",
    "    'constraint_class': 'ScalarRange',\n",
    "    'constraint_parameters': {\n",
    "        'column_name': 'amenities_fee',\n",
    "        'low_value': 0.0,\n",
    "        'high_value': 500.0,\n",
    "        'strict_boundaries': False\n",
    "    }\n",
    "}\n",
    "\n",
    "my_synthesizer.add_constraints(constraints=[\n",
    "    my_constraint\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Sampling conditions: : 350it [00:00, 600.01it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 533.62it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 581.78it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 601.81it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 610.55it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 584.12it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 604.58it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 585.97it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 570.28it/s]                       \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:00, 607.60it/s]                       \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    gc_synthesizer = GaussianCopulaSynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=True,\n",
    "        enforce_rounding=True,\n",
    "        default_distribution='beta'\n",
    "    )\n",
    "    gc_synthesizer.auto_assign_transformers(df)\n",
    "    processed_df = gc_synthesizer.preprocess(df)\n",
    "    gc_synthesizer.fit_processed_data(processed_df)\n",
    "    gc_synthetic_data = gc_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    # Post-process float columns to round to nearest half\n",
    "    gc_synthetic_data['eval-TLP-FigRey-totalCopia-PD'] = gc_synthetic_data['eval-TLP-FigRey-totalCopia-PD'].apply(round_to_nearest_half)\n",
    "    gc_synthetic_data['eval-TLP-FigRey-totalMemoria-PD'] = gc_synthetic_data['eval-TLP-FigRey-totalMemoria-PD'].apply(round_to_nearest_half)\n",
    "\n",
    "    gc_synthetic_data = pd.concat([df, gc_synthetic_data], ignore_index=True)\n",
    "    gc_synthetic_data.to_csv(\n",
    "        f'../../data/synthetic/gc/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Gen. (-6.77) | Discrim. (0.50): 100%|██████████| 1000/1000 [03:28<00:00,  4.80it/s]\n",
      "Sampling conditions: : 300it [00:08, 32.79it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:08, 36.24it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-6.23) | Discrim. (0.53): 100%|██████████| 1000/1000 [03:17<00:00,  5.06it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:04, 38.34it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 297it [00:06, 46.61it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 41.88it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-6.93) | Discrim. (-0.36): 100%|██████████| 1000/1000 [03:32<00:00,  4.70it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 48.07it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 298it [00:06, 44.58it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 42.03it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-5.08) | Discrim. (-0.53): 100%|██████████| 1000/1000 [03:34<00:00,  4.65it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:04, 38.24it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 35.52it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 41.86it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-8.04) | Discrim. (0.05): 100%|██████████| 1000/1000 [03:32<00:00,  4.72it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 46.84it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 39.91it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 43.78it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-4.64) | Discrim. (0.13): 100%|██████████| 1000/1000 [03:35<00:00,  4.65it/s]\n",
      "Sampling conditions:  35%|███▌      | 97/275 [00:02<00:05, 32.41it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:08, 43.47it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:08, 34.89it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-7.49) | Discrim. (0.38): 100%|██████████| 1000/1000 [03:31<00:00,  4.72it/s]\n",
      "Sampling conditions: : 300it [00:06, 53.05it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 47.02it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-7.71) | Discrim. (-0.59): 100%|██████████| 1000/1000 [03:35<00:00,  4.65it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:05, 33.87it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 47.16it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 37.82it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-8.95) | Discrim. (0.04): 100%|██████████| 1000/1000 [03:30<00:00,  4.75it/s] \n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:04, 37.82it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 49.71it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 44.68it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-3.38) | Discrim. (0.83): 100%|██████████| 1000/1000 [03:34<00:00,  4.66it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 53.38it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 43.48it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 44.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases', 'clin-frecUsoEmail'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    ctgan_synthesizer = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=True,\n",
    "        enforce_rounding=True,\n",
    "        locales=['es_ES'],\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        cuda=True\n",
    "    )\n",
    "\n",
    "    ctgan_synthesizer.auto_assign_transformers(df)\n",
    "    processed_df = ctgan_synthesizer.preprocess(df)\n",
    "    ctgan_synthesizer.fit_processed_data(processed_df)\n",
    "    ctgan_synthetic_data = ctgan_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    ctgan_synthetic_data = pd.concat([df, ctgan_synthetic_data], ignore_index=True)\n",
    "    ctgan_synthetic_data.to_csv(\n",
    "        f'../../data/synthetic/ctgan/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. TVAESynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -274.601: 100%|██████████| 5000/5000 [04:20<00:00, 19.20it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 47.70it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 35.77it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 41.90it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -277.875: 100%|██████████| 5000/5000 [04:18<00:00, 19.37it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:04, 38.94it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 53.17it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 42.41it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -290.687: 100%|██████████| 5000/5000 [04:18<00:00, 19.35it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 49.57it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 49.38it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 47.56it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.785: 100%|██████████| 5000/5000 [04:17<00:00, 19.42it/s]\n",
      "Sampling conditions:  34%|███▍      | 94/275 [00:01<00:02, 68.84it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 41.41it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 39.93it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -276.529: 100%|██████████| 5000/5000 [04:14<00:00, 19.62it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 47.26it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 296it [00:07, 41.04it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:08, 37.40it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.755: 100%|██████████| 5000/5000 [04:18<00:00, 19.35it/s]\n",
      "Sampling conditions: : 300it [00:08, 39.29it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:08, 37.10it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -285.812: 100%|██████████| 5000/5000 [04:18<00:00, 19.34it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 48.37it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 41.12it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 45.09it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -285.543: 100%|██████████| 5000/5000 [04:30<00:00, 18.47it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 49.66it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 51.63it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 47.17it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.513: 100%|██████████| 5000/5000 [04:24<00:00, 18.90it/s]\n",
      "Sampling conditions: : 300it [00:09, 34.64it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 30.61it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -279.625: 100%|██████████| 5000/5000 [04:26<00:00, 18.75it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:01<00:03, 47.29it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 41.92it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 48.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases', 'clin-frecUsoEmail'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    tvaes_synthesizer = TVAESynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=True,\n",
    "        enforce_rounding=True,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        cuda=True\n",
    "    )\n",
    "\n",
    "    tvaes_synthesizer.auto_assign_transformers(df)\n",
    "    processed_df = tvaes_synthesizer.preprocess(df)\n",
    "    tvaes_synthesizer.fit_processed_data(processed_df)\n",
    "    tvaes_synthetic_data = tvaes_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    tvaes_synthetic_data = pd.concat([df, tvaes_synthetic_data], ignore_index=True)\n",
    "    tvaes_synthetic_data.to_csv(\n",
    "        f'../../data/synthetic/tvaes/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. CopulaGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Gen. (-9.03) | Discrim. (0.24): 100%|██████████| 5000/5000 [16:45<00:00,  4.97it/s]  \n",
      "Sampling conditions: : 300it [00:09, 34.01it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 32.72it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-14.16) | Discrim. (-0.57): 100%|██████████| 5000/5000 [15:50<00:00,  5.26it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:03, 44.68it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 49.39it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 46.80it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-12.30) | Discrim. (-0.72): 100%|██████████| 5000/5000 [16:33<00:00,  5.03it/s]\n",
      "Sampling conditions:  36%|███▌      | 98/275 [00:02<00:04, 41.40it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 45.71it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 38.18it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-10.89) | Discrim. (-0.36): 100%|██████████| 5000/5000 [16:47<00:00,  4.96it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:04, 35.98it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 43.38it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 39.92it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-11.96) | Discrim. (0.33): 100%|██████████| 5000/5000 [16:44<00:00,  4.98it/s] \n",
      "Sampling conditions: : 300it [00:09, 38.23it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 32.68it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-10.55) | Discrim. (-0.66): 100%|██████████| 5000/5000 [17:15<00:00,  4.83it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:03, 44.96it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 299it [00:08, 33.52it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 30.55it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-11.14) | Discrim. (-0.23): 100%|██████████| 5000/5000 [17:41<00:00,  4.71it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:05, 32.36it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 34.95it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 38.17it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-8.82) | Discrim. (0.41): 100%|██████████| 5000/5000 [17:36<00:00,  4.73it/s]  \n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:04, 43.64it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 298it [00:07, 34.13it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 38.96it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-10.99) | Discrim. (-0.04): 100%|██████████| 5000/5000 [17:20<00:00,  4.80it/s]\n",
      "Sampling conditions: : 300it [00:09, 44.35it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 30.76it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-12.58) | Discrim. (-0.07): 100%|██████████| 5000/5000 [16:55<00:00,  4.92it/s]\n",
      "Sampling conditions:  36%|███▋      | 100/275 [00:02<00:05, 31.33it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 297it [00:07, 36.97it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 37.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases', 'clin-frecUsoEmail'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    cg_synthesizer = CopulaGANSynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=True,\n",
    "        enforce_rounding=True,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        cuda=True\n",
    "    )\n",
    "\n",
    "    cg_synthesizer.auto_assign_transformers(df)\n",
    "    processed_df = cg_synthesizer.preprocess(df)\n",
    "    cg_synthesizer.fit_processed_data(processed_df)\n",
    "    cg_synthetic_data = cg_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    cg_synthetic_data = pd.concat([df, cg_synthetic_data], ignore_index=True)\n",
    "    cg_synthetic_data.to_csv(\n",
    "        f'../../data/synthetic/cg/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating Real vs. Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import get_column_plot\n",
    "\n",
    "plot_columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "\n",
    "gc_diagnostic = run_diagnostic(\n",
    "    real_data=df,\n",
    "    synthetic_data=gc_synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "\n",
    "gc_quality_report = evaluate_quality(\n",
    "    df,\n",
    "    gc_synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_details = gc_quality_report.get_details('Column Pair Trends')\n",
    "gc_details[gc_details['Real Correlation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_quality_report.get_details('Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in plot_columns:\n",
    "     fig = get_column_plot(\n",
    "         real_data=df,\n",
    "         synthetic_data=gc_synthetic_data,\n",
    "         column_name=column,\n",
    "         metadata=metadata\n",
    "     )\n",
    "     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_diagnostic = run_diagnostic(\n",
    "    real_data=df,\n",
    "    synthetic_data=ctgan_synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_quality_report = evaluate_quality(\n",
    "    df,\n",
    "    ctgan_synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_details = ctgan_quality_report.get_details('Column Pair Trends')\n",
    "ctgan_details[ctgan_details['Real Correlation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_quality_report.get_details('Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in plot_columns:\n",
    "    fig = get_column_plot(\n",
    "        real_data=df,\n",
    "        synthetic_data=ctgan_synthetic_data,\n",
    "        column_name=column,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. TVAESSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_diagnostic = run_diagnostic(\n",
    "    real_data=df,\n",
    "    synthetic_data=tvaes_synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_quality_report = evaluate_quality(\n",
    "    df,\n",
    "    tvaes_synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_details = tvaes_quality_report.get_details('Column Pair Trends')\n",
    "tvaes_details[tvaes_details['Real Correlation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_quality_report.get_details('Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in plot_columns:\n",
    "    fig = get_column_plot(\n",
    "        real_data=df,\n",
    "        synthetic_data=tvaes_synthetic_data,\n",
    "        column_name=column,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
