{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.single_table import CopulaGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 50\n",
    "NUM_EPOCHS = 5_000\n",
    "NUM_STOP = 1_100\n",
    "NUM_STEPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, NUM_STOP, NUM_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n",
      "class difference: 75\n",
      "Generating synthetic data for set 0 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 0 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 104.89it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 0 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 106.04it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 0 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 101.88it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 0 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 108.61it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 0 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 110.00it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 0 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 109.75it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 0 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 110.68it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 0 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 110.90it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 0 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 109.35it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 0 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:09, 110.76it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 1\n",
      "class difference: 75\n",
      "Generating synthetic data for set 1 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 1 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 104.50it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 1 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 106.78it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 1 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 108.32it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 1 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 109.69it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 1 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 108.51it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 1 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 110.48it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 1 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 110.36it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 1 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 109.14it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 1 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 109.99it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 1 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:09, 110.94it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 2\n",
      "class difference: 75\n",
      "Generating synthetic data for set 2 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 2 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 105.23it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 2 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 108.71it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 2 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 107.54it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 2 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 109.74it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 2 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 108.91it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 2 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 108.06it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 2 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 101.03it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 2 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:09, 95.90it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 2 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 109.68it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 2 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:09, 110.91it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 3\n",
      "class difference: 75\n",
      "Generating synthetic data for set 3 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 3 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 105.55it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 3 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 103.58it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 3 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 106.24it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 3 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 109.07it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 3 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 109.73it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 3 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 110.53it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 3 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 110.04it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 3 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 108.91it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 3 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 107.47it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 3 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:10, 109.96it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 4\n",
      "class difference: 75\n",
      "Generating synthetic data for set 4 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 4 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 104.05it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 4 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 106.97it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 4 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 108.13it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 4 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 107.56it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 4 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 109.10it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 4 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 109.39it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 4 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 107.81it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 4 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 107.33it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 4 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 109.55it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 4 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:10, 104.19it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 5\n",
      "class difference: 75\n",
      "Generating synthetic data for set 5 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 5 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 103.59it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 5 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 101.89it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 5 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 107.66it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 5 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 107.09it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 5 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 105.14it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 5 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 108.64it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 5 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 107.57it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 5 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 106.83it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 5 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 108.41it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 5 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:10, 107.22it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 6\n",
      "class difference: 75\n",
      "Generating synthetic data for set 6 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 6 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 103.63it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 6 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 103.76it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 6 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 107.83it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 6 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:05, 99.47it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 6 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 106.77it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 6 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 101.56it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 6 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 107.58it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 6 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 110.85it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 6 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 109.54it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 6 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:09, 110.78it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 7\n",
      "class difference: 75\n",
      "Generating synthetic data for set 7 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 7 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 103.87it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 7 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 107.38it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 7 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 108.71it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 7 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 108.53it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 7 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 106.89it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 7 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 106.58it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 7 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 108.28it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 7 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 109.64it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 7 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 109.38it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 7 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:10, 107.78it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 8\n",
      "class difference: 75\n",
      "Generating synthetic data for set 8 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 8 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 103.11it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 8 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:03, 98.65it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 8 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:04, 98.34it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 8 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 105.99it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 8 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 106.96it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 8 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 110.23it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 8 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 107.71it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 8 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 108.70it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 8 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 106.52it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 8 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:09, 110.55it/s]                        \n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 9\n",
      "class difference: 75\n",
      "Generating synthetic data for set 9 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 9 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 200it [00:01, 103.37it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 9 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 300it [00:02, 102.76it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 9 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 400it [00:03, 106.39it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 9 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 500it [00:04, 109.12it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 9 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 600it [00:05, 104.22it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 9 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 700it [00:06, 105.55it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 9 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 800it [00:07, 108.12it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 9 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 900it [00:08, 105.46it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 9 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1000it [00:09, 108.18it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 9 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 1100it [00:09, 112.02it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases', 'clin-frecUsoEmail'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    class_difference = num_d - num_h\n",
    "    print(f'class difference: {class_difference}')\n",
    "    for sync_data in range(0, NUM_STOP, NUM_STEPS):\n",
    "        print(f'Generating synthetic data for set {i} with {sync_data} samples')\n",
    "        if sync_data == 0:\n",
    "            conditions = []\n",
    "        else:\n",
    "            class_d = Condition(\n",
    "                num_rows=int(sync_data/2),\n",
    "                column_values={'ED_2Clases': 'D'}\n",
    "            )\n",
    "            class_h = Condition(\n",
    "                num_rows=int(sync_data/2) + class_difference,\n",
    "                column_values={'ED_2Clases': 'H'}\n",
    "            )\n",
    "            conditions = [class_d, class_h]\n",
    "        gc_synthesizer = GaussianCopulaSynthesizer(\n",
    "            metadata,\n",
    "            enforce_min_max_values=True,\n",
    "            enforce_rounding=True,\n",
    "            locales=[\"es_ES\"],\n",
    "            numerical_distributions={},\n",
    "            default_distribution='beta'\n",
    "        )\n",
    "        gc_synthesizer.auto_assign_transformers(df)\n",
    "        processed_df = gc_synthesizer.preprocess(df)\n",
    "        gc_synthesizer.fit_processed_data(processed_df)\n",
    "        gc_synthetic_data = gc_synthesizer.sample_from_conditions(\n",
    "            conditions=conditions,\n",
    "            batch_size = 50,\n",
    "            max_tries_per_batch = 100\n",
    "        )\n",
    "        gc_synthetic_data = pd.concat([df, gc_synthetic_data], ignore_index=True)\n",
    "        gc_synthetic_data.to_csv(\n",
    "            f'../../data/synthetic/gc/oversampling/set_{i}_over_{sync_data}.csv',\n",
    "            index=False\n",
    "        )\n",
    "        print(f'Original data shape: {df.shape}. Shynthetic data shape: {gc_synthetic_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. TVAESynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n",
      "class difference: 75\n",
      "Generating synthetic data for set 0 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Loss: -277.605: 100%|| 5000/5000 [04:10<00:00, 19.96it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 0 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.826: 100%|| 5000/5000 [04:07<00:00, 20.21it/s]\n",
      "Sampling conditions: : 200it [00:04, 48.76it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 43.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 0 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -276.780: 100%|| 5000/5000 [04:07<00:00, 20.24it/s]\n",
      "Sampling conditions:  36%|      | 98/275 [00:01<00:03, 50.18it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:05, 58.72it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:05, 51.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 0 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.341: 100%|| 5000/5000 [04:06<00:00, 20.26it/s]\n",
      "Sampling conditions:  39%|      | 148/375 [00:03<00:05, 44.09it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:09, 50.80it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:09, 40.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 0 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -270.780: 100%|| 5000/5000 [04:08<00:00, 20.09it/s]\n",
      "Sampling conditions:  42%|     | 200/475 [00:04<00:07, 36.31it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 497it [00:11, 50.51it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:12, 39.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 0 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -272.474: 100%|| 5000/5000 [04:07<00:00, 20.23it/s]\n",
      "Sampling conditions:  43%|     | 250/575 [00:05<00:08, 37.39it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 27.74it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 40.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 0 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.435: 100%|| 5000/5000 [04:06<00:00, 20.32it/s]\n",
      "Sampling conditions:  44%|     | 300/675 [00:06<00:08, 41.94it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:16, 42.71it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:16, 42.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 0 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.683: 100%|| 5000/5000 [04:06<00:00, 20.29it/s]\n",
      "Sampling conditions:  45%|     | 349/775 [00:07<00:10, 42.14it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:17, 59.97it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:17, 46.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 0 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.876: 100%|| 5000/5000 [04:05<00:00, 20.34it/s]\n",
      "Sampling conditions:  46%|     | 400/875 [00:09<00:11, 42.18it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:21, 39.03it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:21, 41.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 0 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.043: 100%|| 5000/5000 [04:01<00:00, 20.68it/s]\n",
      "Sampling conditions:  46%|     | 449/975 [00:09<00:11, 46.08it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 999it [00:24, 34.95it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:24, 40.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 0 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.264: 100%|| 5000/5000 [03:59<00:00, 20.90it/s]\n",
      "Sampling conditions:  47%|     | 500/1075 [00:10<00:13, 41.35it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:25, 50.69it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:25, 43.29it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 1\n",
      "class difference: 75\n",
      "Generating synthetic data for set 1 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -272.494: 100%|| 5000/5000 [04:01<00:00, 20.71it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 1 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -286.229: 100%|| 5000/5000 [03:59<00:00, 20.90it/s]\n",
      "Sampling conditions:  29%|       | 50/175 [00:00<00:02, 51.27it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 48.89it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 48.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 1 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.837: 100%|| 5000/5000 [03:59<00:00, 20.91it/s]\n",
      "Sampling conditions:  36%|      | 100/275 [00:01<00:03, 53.21it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:05, 61.19it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:05, 55.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 1 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -279.578: 100%|| 5000/5000 [03:59<00:00, 20.90it/s]\n",
      "Sampling conditions:  39%|      | 148/375 [00:02<00:04, 48.40it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:10, 27.06it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:10, 38.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 1 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -284.650: 100%|| 5000/5000 [04:08<00:00, 20.09it/s]\n",
      "Sampling conditions: : 500it [00:12, 50.99it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:12, 41.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 1 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -288.777: 100%|| 5000/5000 [04:07<00:00, 20.17it/s]\n",
      "Sampling conditions:  43%|     | 249/575 [00:06<00:10, 32.28it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:17, 46.27it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:17, 33.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 1 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -283.552: 100%|| 5000/5000 [04:08<00:00, 20.13it/s]\n",
      "Sampling conditions:  44%|     | 298/675 [00:06<00:08, 45.13it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:17, 49.90it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:17, 40.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 1 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -285.979: 100%|| 5000/5000 [04:07<00:00, 20.20it/s]\n",
      "Sampling conditions:  45%|     | 349/775 [00:08<00:12, 32.89it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:20, 49.20it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:20, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 1 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -287.287: 100%|| 5000/5000 [04:07<00:00, 20.20it/s]\n",
      "Sampling conditions:  46%|     | 400/875 [00:07<00:10, 44.53it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:19, 39.22it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:19, 45.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 1 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.818: 100%|| 5000/5000 [04:07<00:00, 20.17it/s]\n",
      "Sampling conditions:  46%|     | 449/975 [00:09<00:10, 48.26it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:24, 28.32it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:24, 41.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 1 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -286.550: 100%|| 5000/5000 [04:04<00:00, 20.44it/s]\n",
      "Sampling conditions:  46%|     | 495/1075 [00:08<00:08, 64.82it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:24, 33.42it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:24, 44.32it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 2\n",
      "class difference: 75\n",
      "Generating synthetic data for set 2 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -294.390: 100%|| 5000/5000 [04:03<00:00, 20.58it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 2 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -294.452: 100%|| 5000/5000 [04:02<00:00, 20.58it/s]\n",
      "Sampling conditions:  29%|       | 50/175 [00:00<00:02, 48.84it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 196it [00:04, 41.79it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:05, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 2 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -294.557: 100%|| 5000/5000 [04:02<00:00, 20.62it/s]\n",
      "Sampling conditions:  36%|      | 100/275 [00:01<00:03, 52.66it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 39.79it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 44.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 2 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -290.853: 100%|| 5000/5000 [04:02<00:00, 20.62it/s]\n",
      "Sampling conditions:  40%|      | 150/375 [00:03<00:07, 30.11it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 398it [00:09, 46.56it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:09, 40.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 2 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -291.331: 100%|| 5000/5000 [04:02<00:00, 20.64it/s]\n",
      "Sampling conditions:  42%|     | 200/475 [00:04<00:07, 37.13it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 55.53it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 44.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 2 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -305.089: 100%|| 5000/5000 [04:02<00:00, 20.65it/s]\n",
      "Sampling conditions:  43%|     | 249/575 [00:06<00:07, 42.15it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 55.94it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 41.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 2 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -295.706: 100%|| 5000/5000 [04:02<00:00, 20.65it/s]\n",
      "Sampling conditions:  44%|     | 299/675 [00:05<00:07, 50.69it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 697it [00:15, 44.85it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:16, 42.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 2 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -291.279: 100%|| 5000/5000 [04:01<00:00, 20.67it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:07<00:09, 46.61it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 42.14it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 42.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 2 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -293.783: 100%|| 5000/5000 [04:05<00:00, 20.41it/s]\n",
      "Sampling conditions:  46%|     | 400/875 [00:07<00:09, 50.70it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 898it [00:18, 51.38it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:18, 47.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 2 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -290.805: 100%|| 5000/5000 [04:03<00:00, 20.57it/s]\n",
      "Sampling conditions:  46%|     | 450/975 [00:09<00:13, 39.46it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 30.80it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 42.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 2 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -292.323: 100%|| 5000/5000 [04:02<00:00, 20.63it/s]\n",
      "Sampling conditions:  47%|     | 500/1075 [00:10<00:18, 30.29it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1095it [00:25, 48.74it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:26, 42.15it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 3\n",
      "class difference: 75\n",
      "Generating synthetic data for set 3 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -275.668: 100%|| 5000/5000 [04:00<00:00, 20.78it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 3 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -276.022: 100%|| 5000/5000 [04:01<00:00, 20.67it/s]\n",
      "Sampling conditions: : 199it [00:05, 44.06it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:06, 28.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 3 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -269.200: 100%|| 5000/5000 [04:00<00:00, 20.75it/s]\n",
      "Sampling conditions:  36%|      | 100/275 [00:02<00:05, 31.01it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 36.87it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 39.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 3 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.118: 100%|| 5000/5000 [04:00<00:00, 20.81it/s]\n",
      "Sampling conditions:  40%|      | 150/375 [00:03<00:05, 38.42it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:08, 51.74it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:08, 47.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 3 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.389: 100%|| 5000/5000 [04:01<00:00, 20.72it/s]\n",
      "Sampling conditions:  42%|     | 198/475 [00:05<00:08, 33.64it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:13, 45.75it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:13, 36.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 3 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.364: 100%|| 5000/5000 [04:00<00:00, 20.79it/s]\n",
      "Sampling conditions:  43%|     | 249/575 [00:05<00:08, 39.65it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:13, 43.34it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:13, 43.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 3 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -276.865: 100%|| 5000/5000 [04:00<00:00, 20.76it/s]\n",
      "Sampling conditions:  44%|     | 294/675 [00:04<00:05, 66.70it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 697it [00:15, 46.98it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:16, 42.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 3 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -277.944: 100%|| 5000/5000 [04:00<00:00, 20.82it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:07<00:08, 48.82it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:17, 51.70it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:17, 45.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 3 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -272.927: 100%|| 5000/5000 [03:59<00:00, 20.85it/s]\n",
      "Sampling conditions:  46%|     | 400/875 [00:08<00:10, 45.03it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:19, 51.78it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:19, 45.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 3 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -274.252: 100%|| 5000/5000 [04:00<00:00, 20.79it/s]\n",
      "Sampling conditions:  46%|     | 450/975 [00:08<00:09, 52.83it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 50.96it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 42.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 3 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -265.389: 100%|| 5000/5000 [04:01<00:00, 20.72it/s]\n",
      "Sampling conditions:  46%|     | 498/1075 [00:10<00:12, 45.01it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1095it [00:25, 56.01it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:26, 41.82it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 4\n",
      "class difference: 75\n",
      "Generating synthetic data for set 4 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.172: 100%|| 5000/5000 [03:58<00:00, 20.95it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 4 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.643: 100%|| 5000/5000 [03:59<00:00, 20.89it/s]\n",
      "Sampling conditions:  29%|       | 50/175 [00:00<00:02, 52.91it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 51.35it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 4 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -279.681: 100%|| 5000/5000 [03:59<00:00, 20.91it/s]\n",
      "Sampling conditions:  36%|      | 100/275 [00:01<00:03, 52.85it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 298it [00:07, 35.18it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 37.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 4 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -273.562: 100%|| 5000/5000 [03:58<00:00, 20.95it/s]\n",
      "Sampling conditions:  40%|      | 150/375 [00:02<00:04, 53.54it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:08, 33.24it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:08, 45.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 4 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -270.462: 100%|| 5000/5000 [03:58<00:00, 20.95it/s]\n",
      "Sampling conditions:  42%|     | 200/475 [00:04<00:06, 40.71it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 496it [00:10, 48.86it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 43.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 4 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -273.118: 100%|| 5000/5000 [03:59<00:00, 20.85it/s]\n",
      "Sampling conditions:  43%|     | 250/575 [00:05<00:06, 47.32it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:11, 62.10it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:11, 52.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 4 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.379: 100%|| 5000/5000 [03:59<00:00, 20.89it/s]\n",
      "Sampling conditions:  44%|     | 299/675 [00:06<00:09, 39.31it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:15, 55.31it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:15, 46.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 4 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -275.185: 100%|| 5000/5000 [03:58<00:00, 20.94it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:06<00:08, 52.56it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:16, 52.67it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:16, 47.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 4 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -273.328: 100%|| 5000/5000 [03:59<00:00, 20.88it/s]\n",
      "Sampling conditions:  46%|     | 399/875 [00:08<00:11, 39.79it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:20, 43.53it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:20, 44.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 4 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -276.022: 100%|| 5000/5000 [03:58<00:00, 20.94it/s]\n",
      "Sampling conditions:  46%|     | 449/975 [00:09<00:11, 46.26it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:24, 53.41it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:24, 40.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 4 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -272.657: 100%|| 5000/5000 [03:59<00:00, 20.91it/s]\n",
      "Sampling conditions:  46%|     | 499/1075 [00:10<00:11, 48.39it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:25, 39.10it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:25, 43.88it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 5\n",
      "class difference: 75\n",
      "Generating synthetic data for set 5 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -276.009: 100%|| 5000/5000 [04:15<00:00, 19.57it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 5 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -277.055: 100%|| 5000/5000 [04:10<00:00, 19.95it/s]\n",
      "Sampling conditions: : 200it [00:04, 47.28it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 5 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -275.887: 100%|| 5000/5000 [04:10<00:00, 19.99it/s]\n",
      "Sampling conditions:  36%|      | 99/275 [00:02<00:04, 42.15it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 46.39it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:09, 33.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 5 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -289.316: 100%|| 5000/5000 [04:10<00:00, 19.95it/s]\n",
      "Sampling conditions:  39%|      | 148/375 [00:04<00:07, 30.38it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:09, 57.95it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:10, 39.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 5 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.834: 100%|| 5000/5000 [04:12<00:00, 19.78it/s]\n",
      "Sampling conditions:  42%|     | 200/475 [00:04<00:07, 37.02it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 38.64it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 42.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 5 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -286.309: 100%|| 5000/5000 [04:08<00:00, 20.08it/s]\n",
      "Sampling conditions:  43%|     | 250/575 [00:06<00:11, 28.34it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 51.16it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 41.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 5 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -289.423: 100%|| 5000/5000 [04:08<00:00, 20.13it/s]\n",
      "Sampling conditions:  44%|     | 298/675 [00:07<00:09, 41.81it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:17, 43.38it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:17, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 5 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.402: 100%|| 5000/5000 [04:06<00:00, 20.31it/s]\n",
      "Sampling conditions:  45%|     | 349/775 [00:07<00:08, 47.93it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:19, 39.03it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:19, 40.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 5 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -286.880: 100%|| 5000/5000 [04:01<00:00, 20.72it/s]\n",
      "Sampling conditions:  46%|     | 399/875 [00:08<00:11, 42.40it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:20, 42.41it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:20, 44.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 5 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -271.286: 100%|| 5000/5000 [04:01<00:00, 20.74it/s]\n",
      "Sampling conditions:  46%|     | 448/975 [00:10<00:14, 35.83it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 47.46it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 42.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 5 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -287.235: 100%|| 5000/5000 [04:01<00:00, 20.71it/s]\n",
      "Sampling conditions:  46%|     | 494/1075 [00:09<00:09, 59.01it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1098it [00:22, 48.19it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:23, 47.58it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 6\n",
      "class difference: 75\n",
      "Generating synthetic data for set 6 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.980: 100%|| 5000/5000 [03:59<00:00, 20.88it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 6 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -287.639: 100%|| 5000/5000 [03:59<00:00, 20.84it/s]\n",
      "Sampling conditions: : 200it [00:04, 58.42it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 49.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 6 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.889: 100%|| 5000/5000 [03:59<00:00, 20.84it/s]\n",
      "Sampling conditions: : 300it [00:07, 35.06it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 37.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 6 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -290.675: 100%|| 5000/5000 [03:59<00:00, 20.84it/s]\n",
      "Sampling conditions:  39%|      | 147/375 [00:03<00:04, 45.63it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 397it [00:09, 44.95it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:10, 39.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 6 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.955: 100%|| 5000/5000 [03:59<00:00, 20.88it/s]\n",
      "Sampling conditions:  42%|     | 200/475 [00:04<00:06, 40.56it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:14, 33.94it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:14, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 6 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -293.745: 100%|| 5000/5000 [03:59<00:00, 20.87it/s]\n",
      "Sampling conditions:  43%|     | 248/575 [00:06<00:10, 32.64it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:16, 30.63it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:16, 35.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 6 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -287.181: 100%|| 5000/5000 [03:59<00:00, 20.83it/s]\n",
      "Sampling conditions:  44%|     | 300/675 [00:06<00:09, 38.14it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:16, 54.40it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:16, 42.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 6 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -285.352: 100%|| 5000/5000 [04:00<00:00, 20.78it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:07<00:11, 36.43it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 57.57it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 42.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 6 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -287.244: 100%|| 5000/5000 [04:00<00:00, 20.82it/s]\n",
      "Sampling conditions:  46%|     | 399/875 [00:08<00:11, 42.01it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:21, 33.59it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:21, 42.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 6 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -285.108: 100%|| 5000/5000 [04:00<00:00, 20.81it/s]\n",
      "Sampling conditions:  46%|     | 450/975 [00:11<00:10, 47.91it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:27, 48.75it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:27, 36.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 6 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.038: 100%|| 5000/5000 [03:59<00:00, 20.84it/s]\n",
      "Sampling conditions:  46%|     | 497/1075 [00:10<00:13, 42.95it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1096it [00:27, 37.42it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:27, 40.10it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 7\n",
      "class difference: 75\n",
      "Generating synthetic data for set 7 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.812: 100%|| 5000/5000 [04:02<00:00, 20.63it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 7 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -284.829: 100%|| 5000/5000 [04:02<00:00, 20.66it/s]\n",
      "Sampling conditions:  29%|       | 50/175 [00:00<00:02, 50.60it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 54.87it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 48.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 7 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.904: 100%|| 5000/5000 [04:03<00:00, 20.54it/s]\n",
      "Sampling conditions:  36%|      | 100/275 [00:02<00:04, 40.44it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 35.23it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 41.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 7 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.521: 100%|| 5000/5000 [04:02<00:00, 20.58it/s]\n",
      "Sampling conditions:  40%|      | 150/375 [00:03<00:04, 46.82it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 397it [00:08, 56.26it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:08, 44.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 7 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -274.980: 100%|| 5000/5000 [04:03<00:00, 20.57it/s]\n",
      "Sampling conditions:  42%|     | 198/475 [00:03<00:05, 50.30it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:10, 56.55it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:10, 48.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 7 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -285.838: 100%|| 5000/5000 [04:02<00:00, 20.59it/s]\n",
      "Sampling conditions:  43%|     | 249/575 [00:04<00:06, 51.28it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:13, 39.51it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:13, 44.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 7 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.216: 100%|| 5000/5000 [04:12<00:00, 19.84it/s]\n",
      "Sampling conditions:  44%|     | 300/675 [00:07<00:09, 41.52it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 698it [00:17, 37.01it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:18, 37.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 7 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -279.399: 100%|| 5000/5000 [04:12<00:00, 19.79it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:07<00:09, 43.09it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 48.50it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 44.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 7 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -274.015: 100%|| 5000/5000 [04:12<00:00, 19.77it/s]\n",
      "Sampling conditions:  45%|     | 398/875 [00:10<00:14, 32.56it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:24, 25.29it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:24, 37.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 7 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -283.259: 100%|| 5000/5000 [04:11<00:00, 19.86it/s]\n",
      "Sampling conditions:  46%|     | 449/975 [00:09<00:12, 41.20it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:22, 48.36it/s]                      c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:22, 44.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 7 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.361: 100%|| 5000/5000 [04:11<00:00, 19.85it/s]\n",
      "Sampling conditions:  46%|     | 499/1075 [00:10<00:11, 48.20it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:25, 51.00it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:25, 43.47it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 8\n",
      "class difference: 75\n",
      "Generating synthetic data for set 8 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.020: 100%|| 5000/5000 [04:07<00:00, 20.18it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 8 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.631: 100%|| 5000/5000 [04:07<00:00, 20.19it/s]\n",
      "Sampling conditions:  29%|       | 50/175 [00:00<00:02, 51.10it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 196it [00:03, 57.20it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:04, 44.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 8 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -288.821: 100%|| 5000/5000 [04:00<00:00, 20.83it/s]\n",
      "Sampling conditions:  36%|      | 100/275 [00:02<00:04, 42.40it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 297it [00:05, 58.36it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:06, 44.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 8 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -278.871: 100%|| 5000/5000 [03:59<00:00, 20.85it/s]\n",
      "Sampling conditions:  39%|      | 147/375 [00:03<00:06, 36.70it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:10, 54.20it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:10, 39.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 8 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -279.361: 100%|| 5000/5000 [03:59<00:00, 20.84it/s]\n",
      "Sampling conditions:  42%|     | 199/475 [00:03<00:05, 51.64it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:10, 56.50it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:10, 48.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 8 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -286.567: 100%|| 5000/5000 [04:00<00:00, 20.76it/s]\n",
      "Sampling conditions:  43%|     | 250/575 [00:04<00:06, 51.20it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 43.25it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 42.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 8 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -284.445: 100%|| 5000/5000 [04:00<00:00, 20.75it/s]\n",
      "Sampling conditions:  44%|     | 300/675 [00:07<00:09, 41.05it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:18, 39.51it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:18, 38.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 8 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -287.079: 100%|| 5000/5000 [04:00<00:00, 20.81it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:07<00:08, 48.94it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 42.13it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 43.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 8 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -288.499: 100%|| 5000/5000 [04:00<00:00, 20.77it/s]\n",
      "Sampling conditions:  46%|     | 399/875 [00:08<00:10, 45.03it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:19, 56.78it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:19, 45.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 8 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -288.642: 100%|| 5000/5000 [04:00<00:00, 20.78it/s]\n",
      "Sampling conditions:  46%|     | 449/975 [00:10<00:12, 41.62it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 997it [00:22, 58.44it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:23, 42.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 8 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -294.155: 100%|| 5000/5000 [04:00<00:00, 20.82it/s]\n",
      "Sampling conditions:  46%|     | 496/1075 [00:10<00:13, 43.34it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1095it [00:24, 49.59it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:24, 44.25it/s]\n",
      "c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n",
      "Generating synthetic data for set 9\n",
      "class difference: 75\n",
      "Generating synthetic data for set 9 with 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -271.227: 100%|| 5000/5000 [04:00<00:00, 20.81it/s]\n",
      "Sampling conditions: : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (159, 113)\n",
      "Generating synthetic data for set 9 with 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.869: 100%|| 5000/5000 [04:00<00:00, 20.76it/s]\n",
      "Sampling conditions: : 200it [00:04, 38.91it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 200it [00:05, 39.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (334, 113)\n",
      "Generating synthetic data for set 9 with 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.036: 100%|| 5000/5000 [04:16<00:00, 19.51it/s]\n",
      "Sampling conditions:  36%|      | 99/275 [00:01<00:03, 48.21it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 47.53it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 300it [00:07, 39.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (434, 113)\n",
      "Generating synthetic data for set 9 with 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -279.982: 100%|| 5000/5000 [04:03<00:00, 20.50it/s]\n",
      "Sampling conditions:  40%|      | 150/375 [00:03<00:05, 41.63it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:11, 45.50it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 400it [00:11, 36.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (534, 113)\n",
      "Generating synthetic data for set 9 with 400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -273.585: 100%|| 5000/5000 [04:00<00:00, 20.75it/s]\n",
      "Sampling conditions:  42%|     | 200/475 [00:04<00:05, 45.92it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 47.57it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 500it [00:11, 43.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (634, 113)\n",
      "Generating synthetic data for set 9 with 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -284.692: 100%|| 5000/5000 [04:01<00:00, 20.68it/s]\n",
      "Sampling conditions:  43%|     | 250/575 [00:05<00:06, 48.70it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 52.21it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 600it [00:14, 40.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (734, 113)\n",
      "Generating synthetic data for set 9 with 600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -283.205: 100%|| 5000/5000 [04:01<00:00, 20.71it/s]\n",
      "Sampling conditions:  44%|     | 300/675 [00:05<00:07, 52.46it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:15, 58.21it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 700it [00:15, 45.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (834, 113)\n",
      "Generating synthetic data for set 9 with 700 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -280.138: 100%|| 5000/5000 [04:01<00:00, 20.70it/s]\n",
      "Sampling conditions:  45%|     | 350/775 [00:06<00:10, 42.41it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 39.75it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 800it [00:18, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (934, 113)\n",
      "Generating synthetic data for set 9 with 800 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -282.735: 100%|| 5000/5000 [04:00<00:00, 20.76it/s]\n",
      "Sampling conditions:  45%|     | 398/875 [00:07<00:09, 51.08it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:21, 50.47it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 900it [00:21, 41.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1034, 113)\n",
      "Generating synthetic data for set 9 with 900 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -281.822: 100%|| 5000/5000 [04:01<00:00, 20.73it/s]\n",
      "Sampling conditions:  46%|     | 448/975 [00:10<00:12, 41.40it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 999it [00:21, 59.85it/s]                       c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1000it [00:22, 45.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1134, 113)\n",
      "Generating synthetic data for set 9 with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -286.683: 100%|| 5000/5000 [04:00<00:00, 20.82it/s]\n",
      "Sampling conditions:  47%|     | 500/1075 [00:10<00:13, 43.66it/s]c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1099it [00:26, 42.82it/s]                        c:\\Users\\jgala\\uned\\tfm\\synthetic-data\\.conda\\Lib\\site-packages\\sdv\\single_table\\base.py:915: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[: len(sampled_rows)]\n",
      "Sampling conditions: : 1100it [00:27, 40.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (159, 113). Shynthetic data shape: (1234, 113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases', 'clin-frecUsoEmail'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    class_difference = num_d - num_h\n",
    "    print(f'class difference: {class_difference}')\n",
    "    for sync_data in range(0, NUM_STOP, NUM_STEPS):\n",
    "        print(f'Generating synthetic data for set {i} with {sync_data} samples')\n",
    "        if sync_data == 0:\n",
    "            conditions = []\n",
    "        else:\n",
    "            class_d = Condition(\n",
    "                num_rows=int(sync_data/2),\n",
    "                column_values={'ED_2Clases': 'D'}\n",
    "            )\n",
    "            class_h = Condition(\n",
    "                num_rows=int(sync_data/2) + class_difference,\n",
    "                column_values={'ED_2Clases': 'H'}\n",
    "            )\n",
    "            conditions = [class_d, class_h]\n",
    "        tvaes_synthesizer = TVAESynthesizer(\n",
    "            metadata,\n",
    "            enforce_min_max_values=True,\n",
    "            enforce_rounding=True,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            verbose=True,\n",
    "            cuda=True\n",
    "        )\n",
    "        tvaes_synthesizer.auto_assign_transformers(df)\n",
    "        processed_df = tvaes_synthesizer.preprocess(df)\n",
    "        tvaes_synthesizer.fit_processed_data(processed_df)\n",
    "        tvaes_synthetic_data = tvaes_synthesizer.sample_from_conditions(\n",
    "            conditions=conditions,\n",
    "            batch_size = 50,\n",
    "            max_tries_per_batch = 100\n",
    "        )\n",
    "        tvaes_synthetic_data = pd.concat([df, tvaes_synthetic_data], ignore_index=True)\n",
    "        tvaes_synthetic_data.to_csv(\n",
    "            f'../../data/synthetic/gc/oversampling/set_{i}_over_{sync_data}.csv',\n",
    "            index=False\n",
    "        )\n",
    "        print(f'Original data shape: {df.shape}. Shynthetic data shape: {tvaes_synthetic_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    '../../../data/tlp/Identia_UNED_TLP.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etiq-id</th>\n",
       "      <th>etiq-diagMMSE</th>\n",
       "      <th>etiq-diagExpTLPtext</th>\n",
       "      <th>etiq-diagExpTLPcode</th>\n",
       "      <th>etiq-diagExpTLP</th>\n",
       "      <th>etiq-diagExpTLPtext_R2</th>\n",
       "      <th>etiq-diagExpTLPcode_R2</th>\n",
       "      <th>etiq-diagExpTLP_R2</th>\n",
       "      <th>demo-genero</th>\n",
       "      <th>demo-fechaEvaluacion</th>\n",
       "      <th>demo-fechaNacimiento</th>\n",
       "      <th>demo-rangoEdad</th>\n",
       "      <th>demo-edad</th>\n",
       "      <th>clin-reservaCognitiva_total</th>\n",
       "      <th>clin-reservaCognitiva_idiomas</th>\n",
       "      <th>clin-reservaCognitiva_ocupacion</th>\n",
       "      <th>clin-reservaCognitiva_escolaridad</th>\n",
       "      <th>clin-reservaCognitiva_escolaridadPadres</th>\n",
       "      <th>clin-reservaCognitiva_cursos</th>\n",
       "      <th>clin-reservaCognitiva_formacionMusical</th>\n",
       "      <th>clin-reservaCognitiva_actividadLectora</th>\n",
       "      <th>clin-reservaCognitiva_juegos</th>\n",
       "      <th>clin-ansiedad</th>\n",
       "      <th>clin-depresion</th>\n",
       "      <th>clin-cardiopatiaIsquemica</th>\n",
       "      <th>clin-diabetes</th>\n",
       "      <th>clin-Hipercolesterolemia</th>\n",
       "      <th>clin-Hipertension</th>\n",
       "      <th>clin-enfermedadesAltTiroides</th>\n",
       "      <th>clin-enfermedadesRenal</th>\n",
       "      <th>clin-enfermedadesHepatica</th>\n",
       "      <th>clin-enfermedadesAntecedentesCardiacos</th>\n",
       "      <th>clin-tratCronico</th>\n",
       "      <th>clin-tratPsicologico</th>\n",
       "      <th>clin-tratPsiquiatrico</th>\n",
       "      <th>clin-familiaresAlzheimer</th>\n",
       "      <th>clin-familiaresOtraDemencia</th>\n",
       "      <th>clin-alcohol</th>\n",
       "      <th>clin-fumador</th>\n",
       "      <th>clin-numCigarros</th>\n",
       "      <th>clin-aosSinFumar</th>\n",
       "      <th>clin-tipoAlcohol</th>\n",
       "      <th>clin-entornoUrbano</th>\n",
       "      <th>clin-Covid_vacunado</th>\n",
       "      <th>clin-Covid_numDosis</th>\n",
       "      <th>clin-Covid_pasadoCovid</th>\n",
       "      <th>clin-Covid_numVecesCovid</th>\n",
       "      <th>clin-Covid_sintomaDifRespirar</th>\n",
       "      <th>clin-Covid_sintomaDolMuscular</th>\n",
       "      <th>clin-Covid_sintomaEscalofrios</th>\n",
       "      <th>clin-Covid_sintomaDolGarganta</th>\n",
       "      <th>clin-Covid_sintomaFiebre</th>\n",
       "      <th>clin-Covid_sintomaDolCabeza</th>\n",
       "      <th>clin-Covid_sintomaDiarrea</th>\n",
       "      <th>clin-Covid_sintomaSarpullido</th>\n",
       "      <th>clin-Covid_sintomaPerOlfato</th>\n",
       "      <th>clin-Covid_sintomaPerGusto</th>\n",
       "      <th>clin-Covid_sintomaCansancio</th>\n",
       "      <th>clin-Covid_sentimientoAislado</th>\n",
       "      <th>clin-nivelSociabilidad</th>\n",
       "      <th>clin-nivelActFisica</th>\n",
       "      <th>clin-frecUsoOrdenador</th>\n",
       "      <th>clin-frecUsoMovil</th>\n",
       "      <th>clin-frecUsoTele</th>\n",
       "      <th>clin-frecUsoEmail</th>\n",
       "      <th>clin-frecUsoRSociales</th>\n",
       "      <th>clin-frecOlvidos</th>\n",
       "      <th>clin-consumoAlcohol_UBEsemanal</th>\n",
       "      <th>clin-aosSinFumar_Tipos_y_Rangos</th>\n",
       "      <th>clin-tipoFumador</th>\n",
       "      <th>ques-QuejasMemo-Total-PD</th>\n",
       "      <th>ques-NavEspacial-All-PD</th>\n",
       "      <th>ques-NavEspacial-resultadosWFQ_NO-PD</th>\n",
       "      <th>ques-NavEspacial-resultadosWFQ_SA-PD</th>\n",
       "      <th>ques-NavEspacial-resultadosWFQ_DE-PD</th>\n",
       "      <th>ques-Sus-totalPar-PD</th>\n",
       "      <th>ques-Sus-totalImpar-PD</th>\n",
       "      <th>ques-Sus-total-PD</th>\n",
       "      <th>ques-MMSE-Total-PD</th>\n",
       "      <th>ques-MMSE-concentracion-PD</th>\n",
       "      <th>ques-MMSE-fijacion-PD</th>\n",
       "      <th>ques-MMSE-lenguaje-PD</th>\n",
       "      <th>ques-MMSE-memoria-PD</th>\n",
       "      <th>ques-MMSE-orientacion-PD</th>\n",
       "      <th>ques-MMSE-escolaridad-PD</th>\n",
       "      <th>eval-TLP-CubCorsi-totalDirectos-PD</th>\n",
       "      <th>eval-TLP-CubCorsi-totalDirectos-PZ</th>\n",
       "      <th>eval-TLP-CubCorsi-totalinversos-PD</th>\n",
       "      <th>eval-TLP-CubCorsi-totalinversos-PZ</th>\n",
       "      <th>eval-TLP-CubCorsi-total-PD</th>\n",
       "      <th>eval-TLP-CubCorsi-total-PZ</th>\n",
       "      <th>eval-TLP-FigRey-tipoCopia-PD</th>\n",
       "      <th>eval-TLP-FigRey-totalCopia-PD</th>\n",
       "      <th>eval-TLP-FigRey-totalCopia-PZ</th>\n",
       "      <th>eval-TLP-FigRey-totalMemoria-PD</th>\n",
       "      <th>eval-TLP-FigRey-totalMemoria-PZ</th>\n",
       "      <th>eval-TLP-FigRey-Duracion-PD</th>\n",
       "      <th>eval-TLP-FigRey-Duracion-PZ</th>\n",
       "      <th>eval-TLP-Stroop-color-PD</th>\n",
       "      <th>eval-TLP-Stroop-color-PDC</th>\n",
       "      <th>eval-TLP-Stroop-color-PZ</th>\n",
       "      <th>eval-TLP-Stroop-palabras-PD</th>\n",
       "      <th>eval-TLP-Stroop-palabras-PDC</th>\n",
       "      <th>eval-TLP-Stroop-palabras-PZ</th>\n",
       "      <th>eval-TLP-Stroop-palabrasColor-PD</th>\n",
       "      <th>eval-TLP-Stroop-palabrasColor-PDC</th>\n",
       "      <th>eval-TLP-Stroop-palabrasColor-PZ</th>\n",
       "      <th>eval-TLP-Stroop-interferencia-PDC</th>\n",
       "      <th>eval-TLP-Stroop-interferencia-PZ</th>\n",
       "      <th>eval-TLP-Tavec-1_RI_A1-PD</th>\n",
       "      <th>eval-TLP-Tavec-1_RI_A1-PZ</th>\n",
       "      <th>eval-TLP-Tavec-2_RI_A5-PD</th>\n",
       "      <th>eval-TLP-Tavec-2_RI_A5-PZ</th>\n",
       "      <th>eval-TLP-Tavec-3_RI_AT-PD</th>\n",
       "      <th>eval-TLP-Tavec-3_RI_AT-PZ</th>\n",
       "      <th>eval-TLP-Tavec-4_RI_B-PD</th>\n",
       "      <th>eval-TLP-Tavec-4_RI_B-PZ</th>\n",
       "      <th>eval-TLP-Tavec-5_Rg_Pr-PD</th>\n",
       "      <th>eval-TLP-Tavec-5_Rg_Pr-PZ</th>\n",
       "      <th>eval-TLP-Tavec-6_Rg_Md-PD</th>\n",
       "      <th>eval-TLP-Tavec-6_Rg_Md-PZ</th>\n",
       "      <th>eval-TLP-Tavec-7_Rg_Rc-PD</th>\n",
       "      <th>eval-TLP-Tavec-7_Rg_Rc-PZ</th>\n",
       "      <th>eval-TLP-Tavec-8_RL_CP-PD</th>\n",
       "      <th>eval-TLP-Tavec-8_RL_CP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-9_RCl_CP-PD</th>\n",
       "      <th>eval-TLP-Tavec-9_RCl_CP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-10_RL_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-10_RL_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-11_RCl_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-11_RCl_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-12_ESem_RI_A-PD</th>\n",
       "      <th>eval-TLP-Tavec-12_ESem_RI_A-PZ</th>\n",
       "      <th>eval-TLP-Tavec-13_ESem_RI_B-PD</th>\n",
       "      <th>eval-TLP-Tavec-13_ESem_RI_B-PZ</th>\n",
       "      <th>eval-TLP-Tavec-14_ESem_RL_CP-PD</th>\n",
       "      <th>eval-TLP-Tavec-14_ESem_RL_CP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-15_ESem_RL_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-15_ESem_RL_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-16_ESer_RI_A-PD</th>\n",
       "      <th>eval-TLP-Tavec-16_ESer_RI_A-PZ</th>\n",
       "      <th>eval-TLP-Tavec-17_ESer_RI_B-PD</th>\n",
       "      <th>eval-TLP-Tavec-17_ESer_RI_B-PZ</th>\n",
       "      <th>eval-TLP-Tavec-18_ESer_RL_CP-PD</th>\n",
       "      <th>eval-TLP-Tavec-18_ESer_RL_CP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-19_ESer_RL_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-19_ESer_RL_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-20_P-PD</th>\n",
       "      <th>eval-TLP-Tavec-20_P-PZ</th>\n",
       "      <th>eval-TLP-Tavec-21_I_RL-PD</th>\n",
       "      <th>eval-TLP-Tavec-21_I_RL-PZ</th>\n",
       "      <th>eval-TLP-Tavec-22_I_RCL-PD</th>\n",
       "      <th>eval-TLP-Tavec-22_I_RCL-PZ</th>\n",
       "      <th>eval-TLP-Tavec-23_Recon_Ac-PD</th>\n",
       "      <th>eval-TLP-Tavec-23_Recon_Ac-PZ</th>\n",
       "      <th>eval-TLP-Tavec-24_FP-PD</th>\n",
       "      <th>eval-TLP-Tavec-24_FP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-25_Discriminabilidad-PD</th>\n",
       "      <th>eval-TLP-Tavec-25_Discriminabilidad-PZ</th>\n",
       "      <th>eval-TLP-Tavec-26_Sesgo-PD</th>\n",
       "      <th>eval-TLP-Tavec-26_Sesgo-PZ</th>\n",
       "      <th>eval-TLP-Tavec-27_RI_B_frente_RI_A1-PD</th>\n",
       "      <th>eval-TLP-Tavec-27_RI_B_frente_RI_A1-PZ</th>\n",
       "      <th>eval-TLP-Tavec-28_RL_CP_frente_RI_A5-PD</th>\n",
       "      <th>eval-TLP-Tavec-28_RL_CP_frente_RI_A5-PZ</th>\n",
       "      <th>eval-TLP-Tavec-29_RCl_Cp_frente_RCl_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-29_RCl_Cp_frente_RCl_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-30_RL_LP_frente_RL_CP-PD</th>\n",
       "      <th>eval-TLP-Tavec-30_RL_LP_frente_RL_CP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-31_RCl_LP_frente_RL_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-31_RCl_LP_frente_RL_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-32_Recon_Ac_frente_RL_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-32_Recon_Ac_frente_RL_LP-PZ</th>\n",
       "      <th>eval-TLP-Tavec-33_Recon_Ac_frente_RCl_LP-PD</th>\n",
       "      <th>eval-TLP-Tavec-33_Recon_Ac_frente_RCl_LP-PZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_M_Rango3_056_ED2</td>\n",
       "      <td>ED2</td>\n",
       "      <td>DC MUY LEVE, ejecutivo - atencional</td>\n",
       "      <td>2.07</td>\n",
       "      <td>ED2</td>\n",
       "      <td>DC MUY LEVE, ejecutivo - atencional</td>\n",
       "      <td>2.07</td>\n",
       "      <td>ED2</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-02-22T00:00:00</td>\n",
       "      <td>1942-05-08T00:00:00</td>\n",
       "      <td>Rango3</td>\n",
       "      <td>81</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>8</td>\n",
       "      <td>106</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>92.5</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>253</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>78</td>\n",
       "      <td>89</td>\n",
       "      <td>0.6</td>\n",
       "      <td>80</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-13.715847</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>11</td>\n",
       "      <td>0.48</td>\n",
       "      <td>49</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>32.65</td>\n",
       "      <td>0.29</td>\n",
       "      <td>36.73</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>30.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11</td>\n",
       "      <td>1.01</td>\n",
       "      <td>9</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8</td>\n",
       "      <td>0.14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2</td>\n",
       "      <td>2.60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.22</td>\n",
       "      <td>84.09</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-20.00</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>33.33</td>\n",
       "      <td>1.70</td>\n",
       "      <td>37.50</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-33.33</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-38.46</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-7.69</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_M_Rango3_312_ED3</td>\n",
       "      <td>ED3</td>\n",
       "      <td>DC LEVE, ejecutivo - atencional</td>\n",
       "      <td>3.07</td>\n",
       "      <td>ED3</td>\n",
       "      <td>DC LEVE, multidominio amnsico</td>\n",
       "      <td>3.06</td>\n",
       "      <td>ED3</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-06-26T00:00:00</td>\n",
       "      <td>1948-10-07T00:00:00</td>\n",
       "      <td>Rango3</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>360</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>48</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>25.00</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>47.22</td>\n",
       "      <td>0.41</td>\n",
       "      <td>27.78</td>\n",
       "      <td>0.19</td>\n",
       "      <td>8</td>\n",
       "      <td>0.22</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>26</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15</td>\n",
       "      <td>1.23</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>79.55</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-71.43</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>12.50</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-20.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-20.00</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_M_Rango2_220_ED1</td>\n",
       "      <td>ED1</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ED1</td>\n",
       "      <td>DC MUY LEVE, multidominio amnsico</td>\n",
       "      <td>2.06</td>\n",
       "      <td>ED2</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-14T00:00:00</td>\n",
       "      <td>1952-05-08T00:00:00</td>\n",
       "      <td>Rango2</td>\n",
       "      <td>72</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>100.0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>0.4</td>\n",
       "      <td>39</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.646153</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>14</td>\n",
       "      <td>1.51</td>\n",
       "      <td>56</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7</td>\n",
       "      <td>0.59</td>\n",
       "      <td>32.14</td>\n",
       "      <td>0.18</td>\n",
       "      <td>44.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>23.21</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>13</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>13</td>\n",
       "      <td>1.48</td>\n",
       "      <td>6</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>88.64</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-14.29</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>75.00</td>\n",
       "      <td>1.31</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-20.00</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-16.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-16.67</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_M_Rango1_122_ED1</td>\n",
       "      <td>ED1</td>\n",
       "      <td>DC MUY LEVE, amnsico</td>\n",
       "      <td>2.01</td>\n",
       "      <td>ED2</td>\n",
       "      <td>DC MUY LEVE, amnsico</td>\n",
       "      <td>2.01</td>\n",
       "      <td>ED2</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-10T00:00:00</td>\n",
       "      <td>1964-01-27T00:00:00</td>\n",
       "      <td>Rango1</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fumador</td>\n",
       "      <td>Fumador</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>46</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>80.0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>140</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>85</td>\n",
       "      <td>89</td>\n",
       "      <td>0.6</td>\n",
       "      <td>110</td>\n",
       "      <td>118</td>\n",
       "      <td>0.6</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-10.734299</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.66</td>\n",
       "      <td>14</td>\n",
       "      <td>0.76</td>\n",
       "      <td>65</td>\n",
       "      <td>1.51</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>26.15</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>52.31</td>\n",
       "      <td>1.05</td>\n",
       "      <td>21.54</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>12</td>\n",
       "      <td>0.51</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>26</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4</td>\n",
       "      <td>1.38</td>\n",
       "      <td>90.91</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.79</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>16.67</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>50.00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>-20.00</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-50.00</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-37.50</td>\n",
       "      <td>-1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_M_Rango1_233_ED2</td>\n",
       "      <td>ED2</td>\n",
       "      <td>DC MUY LEVE, multidominio amnsico</td>\n",
       "      <td>2.06</td>\n",
       "      <td>ED2</td>\n",
       "      <td>DC LEVE, multidominio amnsico</td>\n",
       "      <td>3.06</td>\n",
       "      <td>ED3</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-14T00:00:00</td>\n",
       "      <td>1959-09-16T00:00:00</td>\n",
       "      <td>Rango1</td>\n",
       "      <td>64</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>No fumador</td>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>97.5</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>204</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.012657</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>39</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>35.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>25.64</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>38.46</td>\n",
       "      <td>2.35</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>8</td>\n",
       "      <td>0.26</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>86.36</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>66.67</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-33.33</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               etiq-id etiq-diagMMSE                  etiq-diagExpTLPtext  \\\n",
       "0  ID_M_Rango3_056_ED2           ED2  DC MUY LEVE, ejecutivo - atencional   \n",
       "1  ID_M_Rango3_312_ED3           ED3      DC LEVE, ejecutivo - atencional   \n",
       "2  ID_M_Rango2_220_ED1           ED1                               NORMAL   \n",
       "3  ID_M_Rango1_122_ED1           ED1                DC MUY LEVE, amnsico   \n",
       "4  ID_M_Rango1_233_ED2           ED2   DC MUY LEVE, multidominio amnsico   \n",
       "\n",
       "   etiq-diagExpTLPcode etiq-diagExpTLP               etiq-diagExpTLPtext_R2  \\\n",
       "0                 2.07             ED2  DC MUY LEVE, ejecutivo - atencional   \n",
       "1                 3.07             ED3       DC LEVE, multidominio amnsico   \n",
       "2                 1.00             ED1   DC MUY LEVE, multidominio amnsico   \n",
       "3                 2.01             ED2                DC MUY LEVE, amnsico   \n",
       "4                 2.06             ED2       DC LEVE, multidominio amnsico   \n",
       "\n",
       "   etiq-diagExpTLPcode_R2 etiq-diagExpTLP_R2  demo-genero  \\\n",
       "0                    2.07                ED2            2   \n",
       "1                    3.06                ED3            2   \n",
       "2                    2.06                ED2            2   \n",
       "3                    2.01                ED2            2   \n",
       "4                    3.06                ED3            2   \n",
       "\n",
       "  demo-fechaEvaluacion demo-fechaNacimiento demo-rangoEdad  demo-edad  \\\n",
       "0  2024-02-22T00:00:00  1942-05-08T00:00:00         Rango3         81   \n",
       "1  2024-06-26T00:00:00  1948-10-07T00:00:00         Rango3         75   \n",
       "2  2024-05-14T00:00:00  1952-05-08T00:00:00         Rango2         72   \n",
       "3  2024-04-10T00:00:00  1964-01-27T00:00:00         Rango1         60   \n",
       "4  2024-05-14T00:00:00  1959-09-16T00:00:00         Rango1         64   \n",
       "\n",
       "   clin-reservaCognitiva_total  clin-reservaCognitiva_idiomas  \\\n",
       "0                           12                              0   \n",
       "1                            8                              0   \n",
       "2                           11                              0   \n",
       "3                           18                              2   \n",
       "4                           13                              0   \n",
       "\n",
       "   clin-reservaCognitiva_ocupacion  clin-reservaCognitiva_escolaridad  \\\n",
       "0                                1                                  3   \n",
       "1                                1                                  3   \n",
       "2                                0                                  4   \n",
       "3                                1                                  4   \n",
       "4                                1                                  4   \n",
       "\n",
       "   clin-reservaCognitiva_escolaridadPadres  clin-reservaCognitiva_cursos  \\\n",
       "0                                        1                             0   \n",
       "1                                        2                             0   \n",
       "2                                        1                             0   \n",
       "3                                        2                             3   \n",
       "4                                        1                             2   \n",
       "\n",
       "   clin-reservaCognitiva_formacionMusical  \\\n",
       "0                                       1   \n",
       "1                                       1   \n",
       "2                                       1   \n",
       "3                                       1   \n",
       "4                                       1   \n",
       "\n",
       "   clin-reservaCognitiva_actividadLectora  clin-reservaCognitiva_juegos  \\\n",
       "0                                       4                             2   \n",
       "1                                       1                             0   \n",
       "2                                       3                             2   \n",
       "3                                       4                             1   \n",
       "4                                       2                             2   \n",
       "\n",
       "   clin-ansiedad  clin-depresion  clin-cardiopatiaIsquemica  clin-diabetes  \\\n",
       "0          False            True                      False           True   \n",
       "1           True            True                      False          False   \n",
       "2          False           False                      False          False   \n",
       "3          False           False                      False          False   \n",
       "4          False           False                      False          False   \n",
       "\n",
       "   clin-Hipercolesterolemia  clin-Hipertension  clin-enfermedadesAltTiroides  \\\n",
       "0                      True              False                         False   \n",
       "1                     False              False                         False   \n",
       "2                     False              False                         False   \n",
       "3                     False              False                         False   \n",
       "4                     False              False                         False   \n",
       "\n",
       "   clin-enfermedadesRenal  clin-enfermedadesHepatica  \\\n",
       "0                   False                      False   \n",
       "1                   False                      False   \n",
       "2                   False                      False   \n",
       "3                   False                      False   \n",
       "4                   False                      False   \n",
       "\n",
       "   clin-enfermedadesAntecedentesCardiacos  clin-tratCronico  \\\n",
       "0                                   False              True   \n",
       "1                                   False              True   \n",
       "2                                   False              True   \n",
       "3                                   False             False   \n",
       "4                                   False             False   \n",
       "\n",
       "   clin-tratPsicologico  clin-tratPsiquiatrico  clin-familiaresAlzheimer  \\\n",
       "0                 False                  False                      True   \n",
       "1                 False                   True                     False   \n",
       "2                 False                  False                     False   \n",
       "3                 False                  False                     False   \n",
       "4                 False                  False                     False   \n",
       "\n",
       "   clin-familiaresOtraDemencia  clin-alcohol  clin-fumador  clin-numCigarros  \\\n",
       "0                         True         False         False               0.0   \n",
       "1                        False         False         False               0.0   \n",
       "2                         True         False         False               0.0   \n",
       "3                        False         False          True              15.0   \n",
       "4                        False         False         False               0.0   \n",
       "\n",
       "   clin-aosSinFumar clin-tipoAlcohol  clin-entornoUrbano  \\\n",
       "0                NaN              NaN                True   \n",
       "1                NaN              NaN                True   \n",
       "2                NaN              NaN                True   \n",
       "3                NaN              NaN                True   \n",
       "4                NaN              NaN                True   \n",
       "\n",
       "   clin-Covid_vacunado  clin-Covid_numDosis  clin-Covid_pasadoCovid  \\\n",
       "0                 True                    4                   False   \n",
       "1                 True                    4                    True   \n",
       "2                 True                    3                   False   \n",
       "3                 True                    3                   False   \n",
       "4                 True                    3                    True   \n",
       "\n",
       "   clin-Covid_numVecesCovid  clin-Covid_sintomaDifRespirar  \\\n",
       "0                         0                          False   \n",
       "1                         1                          False   \n",
       "2                         0                          False   \n",
       "3                         0                          False   \n",
       "4                         2                          False   \n",
       "\n",
       "   clin-Covid_sintomaDolMuscular  clin-Covid_sintomaEscalofrios  \\\n",
       "0                          False                          False   \n",
       "1                          False                           True   \n",
       "2                          False                          False   \n",
       "3                          False                          False   \n",
       "4                          False                          False   \n",
       "\n",
       "   clin-Covid_sintomaDolGarganta  clin-Covid_sintomaFiebre  \\\n",
       "0                          False                     False   \n",
       "1                          False                      True   \n",
       "2                          False                     False   \n",
       "3                          False                     False   \n",
       "4                           True                      True   \n",
       "\n",
       "   clin-Covid_sintomaDolCabeza  clin-Covid_sintomaDiarrea  \\\n",
       "0                        False                      False   \n",
       "1                        False                      False   \n",
       "2                        False                      False   \n",
       "3                        False                      False   \n",
       "4                        False                      False   \n",
       "\n",
       "   clin-Covid_sintomaSarpullido  clin-Covid_sintomaPerOlfato  \\\n",
       "0                         False                        False   \n",
       "1                         False                        False   \n",
       "2                         False                        False   \n",
       "3                         False                        False   \n",
       "4                         False                        False   \n",
       "\n",
       "   clin-Covid_sintomaPerGusto  clin-Covid_sintomaCansancio  \\\n",
       "0                       False                        False   \n",
       "1                       False                        False   \n",
       "2                       False                        False   \n",
       "3                       False                        False   \n",
       "4                       False                         True   \n",
       "\n",
       "   clin-Covid_sentimientoAislado  clin-nivelSociabilidad  clin-nivelActFisica  \\\n",
       "0                            4.0                       3                    5   \n",
       "1                            4.0                       3                    3   \n",
       "2                            3.0                       3                    1   \n",
       "3                            3.0                       5                    5   \n",
       "4                            3.0                       5                    5   \n",
       "\n",
       "   clin-frecUsoOrdenador  clin-frecUsoMovil  clin-frecUsoTele  \\\n",
       "0                      5                  5                 5   \n",
       "1                      1                  1                 5   \n",
       "2                      5                  5                 5   \n",
       "3                      5                  5                 1   \n",
       "4                      5                  5                 5   \n",
       "\n",
       "   clin-frecUsoEmail  clin-frecUsoRSociales  clin-frecOlvidos  \\\n",
       "0                  1                      3                 4   \n",
       "1                  1                      1                 3   \n",
       "2                  5                      5                 5   \n",
       "3                  3                      3                 3   \n",
       "4                  5                      1                 3   \n",
       "\n",
       "   clin-consumoAlcohol_UBEsemanal clin-aosSinFumar_Tipos_y_Rangos  \\\n",
       "0                             0.0                       No fumador   \n",
       "1                             0.0                       No fumador   \n",
       "2                             0.0                       No fumador   \n",
       "3                             0.0                          Fumador   \n",
       "4                             0.0                       No fumador   \n",
       "\n",
       "  clin-tipoFumador  ques-QuejasMemo-Total-PD  ques-NavEspacial-All-PD  \\\n",
       "0       No fumador                         8                      106   \n",
       "1       No fumador                         8                      110   \n",
       "2       No fumador                         6                       95   \n",
       "3          Fumador                         3                       86   \n",
       "4       No fumador                         6                       86   \n",
       "\n",
       "   ques-NavEspacial-resultadosWFQ_NO-PD  ques-NavEspacial-resultadosWFQ_SA-PD  \\\n",
       "0                                    45                                    49   \n",
       "1                                    52                                    49   \n",
       "2                                    49                                    35   \n",
       "3                                    46                                    31   \n",
       "4                                    41                                    35   \n",
       "\n",
       "   ques-NavEspacial-resultadosWFQ_DE-PD  ques-Sus-totalPar-PD  \\\n",
       "0                                    12                    18   \n",
       "1                                     9                    16   \n",
       "2                                    11                    20   \n",
       "3                                     9                    15   \n",
       "4                                    10                    19   \n",
       "\n",
       "   ques-Sus-totalImpar-PD  ques-Sus-total-PD  ques-MMSE-Total-PD  \\\n",
       "0                      19               92.5                  28   \n",
       "1                      18               85.0                  24   \n",
       "2                      20              100.0                  30   \n",
       "3                      17               80.0                  30   \n",
       "4                      20               97.5                  28   \n",
       "\n",
       "   ques-MMSE-concentracion-PD  ques-MMSE-fijacion-PD  ques-MMSE-lenguaje-PD  \\\n",
       "0                           5                      3                      8   \n",
       "1                           3                      3                      8   \n",
       "2                           5                      3                      9   \n",
       "3                           5                      3                      9   \n",
       "4                           4                      3                      9   \n",
       "\n",
       "   ques-MMSE-memoria-PD  ques-MMSE-orientacion-PD  ques-MMSE-escolaridad-PD  \\\n",
       "0                     2                        10                         1   \n",
       "1                     2                         8                         1   \n",
       "2                     3                        10                         1   \n",
       "3                     3                        10                         1   \n",
       "4                     2                        10                         1   \n",
       "\n",
       "   eval-TLP-CubCorsi-totalDirectos-PD  eval-TLP-CubCorsi-totalDirectos-PZ  \\\n",
       "0                                   6                            0.333333   \n",
       "1                                   4                           -0.666667   \n",
       "2                                   7                            0.666667   \n",
       "3                                   8                            0.333333   \n",
       "4                                   6                           -0.666667   \n",
       "\n",
       "   eval-TLP-CubCorsi-totalinversos-PD  eval-TLP-CubCorsi-totalinversos-PZ  \\\n",
       "0                                   4                            0.000000   \n",
       "1                                   2                           -0.666667   \n",
       "2                                   5                            0.333333   \n",
       "3                                   6                            0.333333   \n",
       "4                                   6                            0.333333   \n",
       "\n",
       "   eval-TLP-CubCorsi-total-PD  eval-TLP-CubCorsi-total-PZ  \\\n",
       "0                          10                    0.000000   \n",
       "1                           6                   -1.000000   \n",
       "2                          12                    0.333333   \n",
       "3                          14                    0.000000   \n",
       "4                          12                   -0.333333   \n",
       "\n",
       "   eval-TLP-FigRey-tipoCopia-PD  eval-TLP-FigRey-totalCopia-PD  \\\n",
       "0                             4                           28.0   \n",
       "1                             4                           14.0   \n",
       "2                             2                           33.0   \n",
       "3                             1                           34.0   \n",
       "4                             1                           36.0   \n",
       "\n",
       "   eval-TLP-FigRey-totalCopia-PZ  eval-TLP-FigRey-totalMemoria-PD  \\\n",
       "0                      -0.333333                             11.0   \n",
       "1                      -1.666667                              1.5   \n",
       "2                       0.333333                             10.0   \n",
       "3                       0.666667                             19.0   \n",
       "4                       2.666667                             12.0   \n",
       "\n",
       "   eval-TLP-FigRey-totalMemoria-PZ  eval-TLP-FigRey-Duracion-PD  \\\n",
       "0                         0.000000                          253   \n",
       "1                        -1.666667                          360   \n",
       "2                        -0.333333                          226   \n",
       "3                         0.333333                          140   \n",
       "4                        -0.333333                          204   \n",
       "\n",
       "   eval-TLP-FigRey-Duracion-PZ  eval-TLP-Stroop-color-PD  \\\n",
       "0                     0.333333                        78   \n",
       "1                    -1.000000                        37   \n",
       "2                     0.000000                        70   \n",
       "3                     0.666667                        85   \n",
       "4                    -0.333333                        66   \n",
       "\n",
       "   eval-TLP-Stroop-color-PDC  eval-TLP-Stroop-color-PZ  \\\n",
       "0                         89                       0.6   \n",
       "1                         48                      -2.2   \n",
       "2                         81                       0.0   \n",
       "3                         89                       0.6   \n",
       "4                         70                      -0.6   \n",
       "\n",
       "   eval-TLP-Stroop-palabras-PD  eval-TLP-Stroop-palabras-PDC  \\\n",
       "0                           80                            94   \n",
       "1                           66                            80   \n",
       "2                          100                           114   \n",
       "3                          110                           118   \n",
       "4                           80                            88   \n",
       "\n",
       "   eval-TLP-Stroop-palabras-PZ  eval-TLP-Stroop-palabrasColor-PD  \\\n",
       "0                         -0.6                                17   \n",
       "1                         -1.4                                14   \n",
       "2                          0.4                                39   \n",
       "3                          0.6                                35   \n",
       "4                         -1.0                                40   \n",
       "\n",
       "   eval-TLP-Stroop-palabrasColor-PDC  eval-TLP-Stroop-palabrasColor-PZ  \\\n",
       "0                                 32                              -1.2   \n",
       "1                                 29                              -1.6   \n",
       "2                                 54                               1.0   \n",
       "3                                 40                              -0.4   \n",
       "4                                 45                               0.0   \n",
       "\n",
       "   eval-TLP-Stroop-interferencia-PDC  eval-TLP-Stroop-interferencia-PZ  \\\n",
       "0                         -13.715847                              -1.4   \n",
       "1                          -1.000000                               0.0   \n",
       "2                           6.646153                               0.8   \n",
       "3                         -10.734299                              -1.0   \n",
       "4                           6.012657                               0.6   \n",
       "\n",
       "   eval-TLP-Tavec-1_RI_A1-PD  eval-TLP-Tavec-1_RI_A1-PZ  \\\n",
       "0                          4                      -0.42   \n",
       "1                          2                      -1.35   \n",
       "2                          6                       0.08   \n",
       "3                          8                       0.66   \n",
       "4                          4                      -1.32   \n",
       "\n",
       "   eval-TLP-Tavec-2_RI_A5-PD  eval-TLP-Tavec-2_RI_A5-PZ  \\\n",
       "0                         11                       0.48   \n",
       "1                          9                      -0.17   \n",
       "2                         14                       1.51   \n",
       "3                         14                       0.76   \n",
       "4                         10                      -1.12   \n",
       "\n",
       "   eval-TLP-Tavec-3_RI_AT-PD  eval-TLP-Tavec-3_RI_AT-PZ  \\\n",
       "0                         49                       0.89   \n",
       "1                         36                      -0.19   \n",
       "2                         56                       1.41   \n",
       "3                         65                       1.51   \n",
       "4                         39                      -1.25   \n",
       "\n",
       "   eval-TLP-Tavec-4_RI_B-PD  eval-TLP-Tavec-4_RI_B-PZ  \\\n",
       "0                         5                     -0.03   \n",
       "1                         7                      0.92   \n",
       "2                         7                      0.59   \n",
       "3                         5                     -0.35   \n",
       "4                         4                     -0.81   \n",
       "\n",
       "   eval-TLP-Tavec-5_Rg_Pr-PD  eval-TLP-Tavec-5_Rg_Pr-PZ  \\\n",
       "0                      32.65                       0.29   \n",
       "1                      25.00                      -0.91   \n",
       "2                      32.14                       0.18   \n",
       "3                      26.15                      -0.67   \n",
       "4                      35.90                       1.08   \n",
       "\n",
       "   eval-TLP-Tavec-6_Rg_Md-PD  eval-TLP-Tavec-6_Rg_Md-PZ  \\\n",
       "0                      36.73                      -0.58   \n",
       "1                      47.22                       0.41   \n",
       "2                      44.64                       0.01   \n",
       "3                      52.31                       1.05   \n",
       "4                      25.64                      -2.95   \n",
       "\n",
       "   eval-TLP-Tavec-7_Rg_Rc-PD  eval-TLP-Tavec-7_Rg_Rc-PZ  \\\n",
       "0                      30.61                       0.53   \n",
       "1                      27.78                       0.19   \n",
       "2                      23.21                      -0.14   \n",
       "3                      21.54                      -0.45   \n",
       "4                      38.46                       2.35   \n",
       "\n",
       "   eval-TLP-Tavec-8_RL_CP-PD  eval-TLP-Tavec-8_RL_CP-PZ  \\\n",
       "0                         11                       1.01   \n",
       "1                          8                       0.22   \n",
       "2                          8                      -0.45   \n",
       "3                         12                       0.51   \n",
       "4                          6                      -1.64   \n",
       "\n",
       "   eval-TLP-Tavec-9_RCl_CP-PD  eval-TLP-Tavec-9_RCl_CP-PZ  \\\n",
       "0                           9                        0.07   \n",
       "1                           8                       -0.23   \n",
       "2                           8                       -0.95   \n",
       "3                          10                       -0.69   \n",
       "4                           9                       -1.07   \n",
       "\n",
       "   eval-TLP-Tavec-10_RL_LP-PD  eval-TLP-Tavec-10_RL_LP-PZ  \\\n",
       "0                           8                        0.14   \n",
       "1                           8                        0.14   \n",
       "2                          10                        0.14   \n",
       "3                           8                       -1.13   \n",
       "4                           9                       -0.78   \n",
       "\n",
       "   eval-TLP-Tavec-11_RCl_LP-PD  eval-TLP-Tavec-11_RCl_LP-PZ  \\\n",
       "0                           12                         0.95   \n",
       "1                            8                        -0.19   \n",
       "2                           10                        -0.25   \n",
       "3                           10                        -0.77   \n",
       "4                            9                        -1.14   \n",
       "\n",
       "   eval-TLP-Tavec-12_ESem_RI_A-PD  eval-TLP-Tavec-12_ESem_RI_A-PZ  \\\n",
       "0                              12                            0.64   \n",
       "1                               7                           -0.25   \n",
       "2                              11                           -0.05   \n",
       "3                              26                            1.33   \n",
       "4                               6                           -0.88   \n",
       "\n",
       "   eval-TLP-Tavec-13_ESem_RI_B-PD  eval-TLP-Tavec-13_ESem_RI_B-PZ  \\\n",
       "0                               2                            1.25   \n",
       "1                               1                            0.21   \n",
       "2                               1                           -0.01   \n",
       "3                               2                            0.74   \n",
       "4                               1                           -0.09   \n",
       "\n",
       "   eval-TLP-Tavec-14_ESem_RL_CP-PD  eval-TLP-Tavec-14_ESem_RL_CP-PZ  \\\n",
       "0                                3                             0.51   \n",
       "1                                2                             0.04   \n",
       "2                                1                            -0.82   \n",
       "3                                6                             0.84   \n",
       "4                                0                            -1.46   \n",
       "\n",
       "   eval-TLP-Tavec-15_ESem_RL_LP-PD  eval-TLP-Tavec-15_ESem_RL_LP-PZ  \\\n",
       "0                                0                            -1.06   \n",
       "1                                2                            -0.25   \n",
       "2                                3                            -0.24   \n",
       "3                                5                             0.09   \n",
       "4                                2                            -0.90   \n",
       "\n",
       "   eval-TLP-Tavec-16_ESer_RI_A-PD  eval-TLP-Tavec-16_ESer_RI_A-PZ  \\\n",
       "0                               3                           -0.31   \n",
       "1                               4                            0.04   \n",
       "2                              13                            3.07   \n",
       "3                               5                           -0.09   \n",
       "4                               4                           -0.30   \n",
       "\n",
       "   eval-TLP-Tavec-17_ESer_RI_B-PD  eval-TLP-Tavec-17_ESer_RI_B-PZ  \\\n",
       "0                               0                           -0.42   \n",
       "1                               1                            0.66   \n",
       "2                               2                            1.96   \n",
       "3                               0                           -0.55   \n",
       "4                               0                           -0.55   \n",
       "\n",
       "   eval-TLP-Tavec-18_ESer_RL_CP-PD  eval-TLP-Tavec-18_ESer_RL_CP-PZ  \\\n",
       "0                                2                             1.91   \n",
       "1                                1                             0.66   \n",
       "2                                0                            -0.72   \n",
       "3                                0                            -0.63   \n",
       "4                                1                             0.17   \n",
       "\n",
       "   eval-TLP-Tavec-19_ESer_RL_LP-PD  eval-TLP-Tavec-19_ESer_RL_LP-PZ  \\\n",
       "0                                2                             2.60   \n",
       "1                                0                            -0.48   \n",
       "2                                0                            -0.85   \n",
       "3                                0                            -0.63   \n",
       "4                                1                             0.22   \n",
       "\n",
       "   eval-TLP-Tavec-20_P-PD  eval-TLP-Tavec-20_P-PZ  eval-TLP-Tavec-21_I_RL-PD  \\\n",
       "0                       9                    0.26                          2   \n",
       "1                      26                    2.10                         15   \n",
       "2                      13                    1.48                          6   \n",
       "3                       1                   -1.03                          5   \n",
       "4                       8                    0.26                          3   \n",
       "\n",
       "   eval-TLP-Tavec-21_I_RL-PZ  eval-TLP-Tavec-22_I_RCL-PD  \\\n",
       "0                      -0.80                           6   \n",
       "1                       1.23                           6   \n",
       "2                       0.18                           6   \n",
       "3                       0.25                           3   \n",
       "4                      -0.20                           0   \n",
       "\n",
       "   eval-TLP-Tavec-22_I_RCL-PZ  eval-TLP-Tavec-23_Recon_Ac-PD  \\\n",
       "0                       -0.09                             13   \n",
       "1                       -0.09                             10   \n",
       "2                        0.55                             12   \n",
       "3                        0.16                             16   \n",
       "4                       -0.64                             10   \n",
       "\n",
       "   eval-TLP-Tavec-23_Recon_Ac-PZ  eval-TLP-Tavec-24_FP-PD  \\\n",
       "0                          -0.15                        4   \n",
       "1                          -1.46                        3   \n",
       "2                          -0.97                        1   \n",
       "3                           0.82                        4   \n",
       "4                          -2.93                        0   \n",
       "\n",
       "   eval-TLP-Tavec-24_FP-PZ  eval-TLP-Tavec-25_Discriminabilidad-PD  \\\n",
       "0                     0.22                                   84.09   \n",
       "1                    -0.13                                   79.55   \n",
       "2                    -0.52                                   88.64   \n",
       "3                     1.38                                   90.91   \n",
       "4                    -0.73                                   86.36   \n",
       "\n",
       "   eval-TLP-Tavec-25_Discriminabilidad-PZ  eval-TLP-Tavec-26_Sesgo-PD  \\\n",
       "0                                   -0.24                        0.14   \n",
       "1                                   -0.76                       -0.33   \n",
       "2                                   -0.11                       -0.60   \n",
       "3                                   -0.56                        0.60   \n",
       "4                                   -1.39                       -0.71   \n",
       "\n",
       "   eval-TLP-Tavec-26_Sesgo-PZ  eval-TLP-Tavec-27_RI_B_frente_RI_A1-PD  \\\n",
       "0                        0.20                                  -20.00   \n",
       "1                       -0.83                                  -71.43   \n",
       "2                       -1.50                                  -14.29   \n",
       "3                        1.79                                   60.00   \n",
       "4                       -2.19                                    0.00   \n",
       "\n",
       "   eval-TLP-Tavec-27_RI_B_frente_RI_A1-PZ  \\\n",
       "0                                   -0.52   \n",
       "1                                   -1.49   \n",
       "2                                   -0.49   \n",
       "3                                    0.57   \n",
       "4                                   -0.54   \n",
       "\n",
       "   eval-TLP-Tavec-28_RL_CP_frente_RI_A5-PD  \\\n",
       "0                                     0.00   \n",
       "1                                    12.50   \n",
       "2                                    75.00   \n",
       "3                                    16.67   \n",
       "4                                    66.67   \n",
       "\n",
       "   eval-TLP-Tavec-28_RL_CP_frente_RI_A5-PZ  \\\n",
       "0                                    -0.70   \n",
       "1                                    -0.53   \n",
       "2                                     1.31   \n",
       "3                                    -0.19   \n",
       "4                                     1.45   \n",
       "\n",
       "   eval-TLP-Tavec-29_RCl_Cp_frente_RCl_LP-PD  \\\n",
       "0                                      33.33   \n",
       "1                                       0.00   \n",
       "2                                      25.00   \n",
       "3                                       0.00   \n",
       "4                                       0.00   \n",
       "\n",
       "   eval-TLP-Tavec-29_RCl_Cp_frente_RCl_LP-PZ  \\\n",
       "0                                       1.70   \n",
       "1                                       0.63   \n",
       "2                                       1.94   \n",
       "3                                       0.57   \n",
       "4                                       0.57   \n",
       "\n",
       "   eval-TLP-Tavec-30_RL_LP_frente_RL_CP-PD  \\\n",
       "0                                    37.50   \n",
       "1                                     0.00   \n",
       "2                                   -20.00   \n",
       "3                                    50.00   \n",
       "4                                   -33.33   \n",
       "\n",
       "   eval-TLP-Tavec-30_RL_LP_frente_RL_CP-PZ  \\\n",
       "0                                     1.07   \n",
       "1                                     0.12   \n",
       "2                                    -0.78   \n",
       "3                                     2.95   \n",
       "4                                    -1.60   \n",
       "\n",
       "   eval-TLP-Tavec-31_RCl_LP_frente_RL_LP-PD  \\\n",
       "0                                    -33.33   \n",
       "1                                      0.00   \n",
       "2                                      0.00   \n",
       "3                                    -20.00   \n",
       "4                                      0.00   \n",
       "\n",
       "   eval-TLP-Tavec-31_RCl_LP_frente_RL_LP-PZ  \\\n",
       "0                                     -0.55   \n",
       "1                                      0.43   \n",
       "2                                      0.46   \n",
       "3                                     -0.71   \n",
       "4                                      0.29   \n",
       "\n",
       "   eval-TLP-Tavec-32_Recon_Ac_frente_RL_LP-PD  \\\n",
       "0                                      -38.46   \n",
       "1                                      -20.00   \n",
       "2                                      -16.67   \n",
       "3                                      -50.00   \n",
       "4                                      -10.00   \n",
       "\n",
       "   eval-TLP-Tavec-32_Recon_Ac_frente_RL_LP-PZ  \\\n",
       "0                                        0.23   \n",
       "1                                        0.92   \n",
       "2                                        0.69   \n",
       "3                                       -1.54   \n",
       "4                                        0.78   \n",
       "\n",
       "   eval-TLP-Tavec-33_Recon_Ac_frente_RCl_LP-PD  \\\n",
       "0                                        -7.69   \n",
       "1                                       -20.00   \n",
       "2                                       -16.67   \n",
       "3                                       -37.50   \n",
       "4                                       -10.00   \n",
       "\n",
       "   eval-TLP-Tavec-33_Recon_Ac_frente_RCl_LP-PZ  \n",
       "0                                         1.19  \n",
       "1                                         0.65  \n",
       "2                                         0.30  \n",
       "3                                        -1.28  \n",
       "4                                         0.50  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfor_ed(row):\n",
    "   if row['etiq-diagExpTLP_R2'] == 'ED1':\n",
    "      return 'H'\n",
    "   elif row['etiq-diagExpTLP_R2'] == 'ED2' or row['etiq-diagExpTLP_R2'] == 'ED3':\n",
    "      return 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ED_2Clases'] = df.apply(transfor_ed, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    'etiq-diagExpTLPcode',\n",
    "    'etiq-diagExpTLPtext',\n",
    "    'etiq-diagExpTLPcode_R2',\n",
    "    'etiq-diagMMSE',\n",
    "    'etiq-diagExpTLP',\n",
    "    'etiq-diagExpTLP_R2',\n",
    "    'etiq-diagExpTLPtext_R2',\n",
    "    'etiq-id',\n",
    "    'clin-Covid_numDosis',\n",
    "    'clin-Covid_numVecesCovid',\n",
    "    'clin-Covid_pasadoCovid',\n",
    "    'clin-Covid_sentimientoAislado',\n",
    "    'clin-Covid_sintomaCansancio',\n",
    "    'clin-Covid_sintomaDiarrea',\n",
    "    'clin-Covid_sintomaDifRespirar',\n",
    "    'clin-Covid_sintomaDolCabeza',\n",
    "    'clin-Covid_sintomaDolGarganta',\n",
    "    'clin-Covid_sintomaDolMuscular',\n",
    "    'clin-Covid_sintomaEscalofrios',\n",
    "    'clin-Covid_sintomaFiebre',\n",
    "    'clin-Covid_sintomaPerGusto',\n",
    "    'clin-Covid_sintomaPerOlfato',\n",
    "    'clin-Covid_vacunado',\n",
    "    'clin-aosSinFumar_Tipos_y_Rangos',\n",
    "    'clin-consumoAlcohol_UBEsemanal',\n",
    "    'clin-enfermedadesAltTiroides',\n",
    "    'clin-enfermedadesAntecedentesCardiacos',\n",
    "    'clin-enfermedadesHepatica',\n",
    "    'clin-enfermedadesRenal',\n",
    "    'clin-entornoUrbano',\n",
    "    'clin-familiaresAlzheimer',\n",
    "    'clin-familiaresOtraDemencia',\n",
    "    'clin-tipoFumador',\n",
    "    'clin-tratCronico',\n",
    "    'clin-tratPsicologico',\n",
    "    'clin-tratPsiquiatrico',\n",
    "    'clin-frecOlvidos',\n",
    "    'clin-frecUsoEmail',\n",
    "    'clin-frecUsoMovil',\n",
    "    'clin-frecUsoOrdenador',\n",
    "    'clin-frecUsoRSociales',\n",
    "    'clin-frecUsoTele',\n",
    "    'clin-nivelActFisica',\n",
    "    'clin-nivelSociabilidad',\n",
    "    'clin-reservaCognitiva_actividadLectora',\n",
    "    'clin-reservaCognitiva_cursos',\n",
    "    'clin-reservaCognitiva_escolaridadPadres',\n",
    "    'clin-reservaCognitiva_formacionMusical',\n",
    "    'clin-reservaCognitiva_juegos',\n",
    "    'clin-numCigarros',\n",
    "    'clin-aosSinFumar',\n",
    "    'clin-tipoAlcohol'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_columns = [column for column in df.columns if 'PT' in column ]\n",
    "pz_columns = [column for column in df.columns if 'PZ' in column ]\n",
    "pdc_columns = [column for column in df.columns if 'PDC' in column]\n",
    "drop_columns += pt_columns + pz_columns + pdc_columns\n",
    "df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 77)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.astype(str).apply(lambda col: col.isin([\"inf\", \"-inf\"]).any())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "\n",
    "metadata = Metadata.detect_from_dataframe(data=df, table_name='TLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'ED_2Clases'\n",
    "]\n",
    "metadata.update_columns(\n",
    "    column_names=categorical_columns,\n",
    "    sdtype='categorical',\n",
    "    table_name='TLP'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto detected data:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (0)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"371pt\" height=\"1337pt\"\n",
       " viewBox=\"0.00 0.00 370.50 1336.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1332.5)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1332.5 366.5,-1332.5 366.5,4 -4,4\"/>\n",
       "<!-- TLP -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>TLP</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M12,-0.5C12,-0.5 350.5,-0.5 350.5,-0.5 356.5,-0.5 362.5,-6.5 362.5,-12.5 362.5,-12.5 362.5,-1316 362.5,-1316 362.5,-1322 356.5,-1328 350.5,-1328 350.5,-1328 12,-1328 12,-1328 6,-1328 0,-1322 0,-1316 0,-1316 0,-12.5 0,-12.5 0,-6.5 6,-0.5 12,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.25\" y=\"-1310.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">TLP</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-1303.5 362.5,-1303.5\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1286.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;genero : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1269.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;fechaEvaluacion : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1253.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;fechaNacimiento : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1236.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;rangoEdad : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1220.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;edad : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1203.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_total : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1187.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_idiomas : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1170.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_ocupacion : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1154.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_escolaridad : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1137.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;ansiedad : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1121.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;depresion : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1104.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;cardiopatiaIsquemica : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1088.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;diabetes : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1071.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;Hipercolesterolemia : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1055.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;Hipertension : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1038.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;alcohol : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1022.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;fumador : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1005.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;Covid_sintomaSarpullido : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-989.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;QuejasMemo&#45;Total&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-972.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;All&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-956.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;resultadosWFQ_NO&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-939.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;resultadosWFQ_SA&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-923.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;resultadosWFQ_DE&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-906.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;Sus&#45;totalPar&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-890.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;Sus&#45;totalImpar&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-873.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;Sus&#45;total&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-857.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;Total&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-840.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;concentracion&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-824.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;fijacion&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-807.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;lenguaje&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-791.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;memoria&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-774.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;orientacion&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-758.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;escolaridad&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-741.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;CubCorsi&#45;totalDirectos&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-725.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;CubCorsi&#45;totalinversos&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-708.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;CubCorsi&#45;total&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-692.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;tipoCopia&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-675.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;totalCopia&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-659.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;totalMemoria&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-642.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;Duracion&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-626.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Stroop&#45;color&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-609.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Stroop&#45;palabras&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-593.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Stroop&#45;palabrasColor&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-576.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;1_RI_A1&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-560.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;2_RI_A5&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-543.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;3_RI_AT&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-527.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;4_RI_B&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-510.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;5_Rg_Pr&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-494.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;6_Rg_Md&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-477.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;7_Rg_Rc&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-461.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;8_RL_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-444.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;9_RCl_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-428.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;10_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-411.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;11_RCl_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-395.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;12_ESem_RI_A&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-378.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;13_ESem_RI_B&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-362.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;14_ESem_RL_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-345.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;15_ESem_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-329.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;16_ESer_RI_A&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-312.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;17_ESer_RI_B&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-296.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;18_ESer_RL_CP&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-279.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;19_ESer_RL_LP&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-263.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;20_P&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-246.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;21_I_RL&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-230.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;22_I_RCL&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-213.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;23_Recon_Ac&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-197.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;24_FP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-180.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;25_Discriminabilidad&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-164.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;26_Sesgo&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-147.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;27_RI_B_frente_RI_A1&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-131.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;28_RL_CP_frente_RI_A5&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-114.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;29_RCl_Cp_frente_RCl_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-98.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;30_RL_LP_frente_RL_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-81.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;31_RCl_LP_frente_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-65.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;32_Recon_Ac_frente_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-48.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;33_Recon_Ac_frente_RCl_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-32.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ED_2Clases : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-25 362.5,-25\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: None</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1e1956d2990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Auto detected data:\\n')\n",
    "metadata.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.validate_table(data=df, table_name='TLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (0)\n",
       " -->\n",
       "<!-- Title: Metadata Pages: 1 -->\n",
       "<svg width=\"371pt\" height=\"1337pt\"\n",
       " viewBox=\"0.00 0.00 370.50 1336.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1332.5)\">\n",
       "<title>Metadata</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1332.5 366.5,-1332.5 366.5,4 -4,4\"/>\n",
       "<!-- TLP -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>TLP</title>\n",
       "<path fill=\"#ffec8b\" stroke=\"black\" d=\"M12,-0.5C12,-0.5 350.5,-0.5 350.5,-0.5 356.5,-0.5 362.5,-6.5 362.5,-12.5 362.5,-12.5 362.5,-1316 362.5,-1316 362.5,-1322 356.5,-1328 350.5,-1328 350.5,-1328 12,-1328 12,-1328 6,-1328 0,-1322 0,-1316 0,-1316 0,-12.5 0,-12.5 0,-6.5 6,-0.5 12,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.25\" y=\"-1310.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">TLP</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-1303.5 362.5,-1303.5\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1286.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;genero : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1269.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;fechaEvaluacion : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1253.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;fechaNacimiento : datetime</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1236.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;rangoEdad : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1220.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">demo&#45;edad : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1203.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_total : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1187.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_idiomas : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1170.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_ocupacion : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1154.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;reservaCognitiva_escolaridad : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1137.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;ansiedad : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1121.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;depresion : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1104.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;cardiopatiaIsquemica : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1088.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;diabetes : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1071.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;Hipercolesterolemia : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1055.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;Hipertension : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1038.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;alcohol : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1022.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;fumador : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1005.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">clin&#45;Covid_sintomaSarpullido : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-989.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;QuejasMemo&#45;Total&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-972.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;All&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-956.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;resultadosWFQ_NO&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-939.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;resultadosWFQ_SA&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-923.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;NavEspacial&#45;resultadosWFQ_DE&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-906.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;Sus&#45;totalPar&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-890.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;Sus&#45;totalImpar&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-873.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;Sus&#45;total&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-857.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;Total&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-840.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;concentracion&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-824.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;fijacion&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-807.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;lenguaje&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-791.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;memoria&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-774.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;orientacion&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-758.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ques&#45;MMSE&#45;escolaridad&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-741.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;CubCorsi&#45;totalDirectos&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-725.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;CubCorsi&#45;totalinversos&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-708.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;CubCorsi&#45;total&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-692.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;tipoCopia&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-675.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;totalCopia&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-659.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;totalMemoria&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-642.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;FigRey&#45;Duracion&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-626.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Stroop&#45;color&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-609.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Stroop&#45;palabras&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-593.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Stroop&#45;palabrasColor&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-576.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;1_RI_A1&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-560.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;2_RI_A5&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-543.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;3_RI_AT&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-527.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;4_RI_B&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-510.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;5_Rg_Pr&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-494.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;6_Rg_Md&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-477.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;7_Rg_Rc&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-461.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;8_RL_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-444.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;9_RCl_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-428.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;10_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-411.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;11_RCl_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-395.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;12_ESem_RI_A&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-378.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;13_ESem_RI_B&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-362.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;14_ESem_RL_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-345.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;15_ESem_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-329.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;16_ESer_RI_A&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-312.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;17_ESer_RI_B&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-296.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;18_ESer_RL_CP&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-279.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;19_ESer_RL_LP&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-263.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;20_P&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-246.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;21_I_RL&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-230.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;22_I_RCL&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-213.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;23_Recon_Ac&#45;PD : categorical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-197.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;24_FP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-180.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;25_Discriminabilidad&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-164.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;26_Sesgo&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-147.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;27_RI_B_frente_RI_A1&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-131.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;28_RL_CP_frente_RI_A5&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-114.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;29_RCl_Cp_frente_RCl_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-98.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;30_RL_LP_frente_RL_CP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-81.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;31_RCl_LP_frente_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-65.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;32_Recon_Ac_frente_RL_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-48.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">eval&#45;TLP&#45;Tavec&#45;33_Recon_Ac_frente_RCl_LP&#45;PD : numerical</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-32.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ED_2Clases : categorical</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-25 362.5,-25\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Primary key: None</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1e18a9dae90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"metadata.json\"):\n",
    "  os.remove(\"metadata.json\")\n",
    "\n",
    "metadata.save_to_json('metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Synthesizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "num_synthetic_data = num_d - num_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ED_2Clases\n",
       "D    248\n",
       "H     66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ED_2Clases'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.sampling import Condition\n",
    "\n",
    "class_d = Condition(\n",
    "    num_rows=3000,\n",
    "    column_values={'ED_2Clases': 'D'}\n",
    ")\n",
    "\n",
    "class_h = Condition(\n",
    "    num_rows=3000,\n",
    "    column_values={'ED_2Clases': 'H'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "\n",
    "gc_synthesizer = GaussianCopulaSynthesizer(\n",
    "    metadata,\n",
    "    enforce_min_max_values=True,\n",
    "    enforce_rounding=True,\n",
    "    locales=[\"es_ES\"],\n",
    "    numerical_distributions={},\n",
    "    default_distribution='beta'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_synthesizer.auto_assign_transformers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = gc_synthesizer.preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_synthesizer.fit_processed_data(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_list = []\n",
    "\n",
    "while len(synthetic_data_list) < num_synthetic_data:\n",
    "    samples = gc_synthesizer.sample(num_rows=NUM_ROWS)\n",
    "    synthetic_data_list.extend(samples.loc[samples['ED_2Clases'] == 'H'].values.tolist()[:num_synthetic_data - len(synthetic_data_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_df = pd.DataFrame(synthetic_data_list, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|| 6000/6000 [00:36<00:00, 162.25it/s]\n"
     ]
    }
   ],
   "source": [
    "gc_synthetic_data_oversampling = gc_synthesizer.sample_from_conditions(\n",
    "    conditions=[class_d, class_h],\n",
    "    batch_size = 50,\n",
    "    max_tries_per_batch = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_synthetic_data = pd.concat([df, synthetic_data_df, gc_synthetic_data_oversampling], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6496, 77)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc_synthetic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_synthetic_data.to_csv(\n",
    "    '../../../data/tlp/synthetic-oversampling-gc.csv',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "ctgan_synthesizer = CTGANSynthesizer(\n",
    "    metadata,\n",
    "    enforce_min_max_values=True,\n",
    "    enforce_rounding=True,\n",
    "    locales=['es_ES'],\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=True,\n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_synthesizer.auto_assign_transformers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = ctgan_synthesizer.preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-1.46) | Discrim. (-0.67): 100%|| 10000/10000 [20:29<00:00,  8.13it/s]\n"
     ]
    }
   ],
   "source": [
    "ctgan_synthesizer.fit_processed_data(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_list = []\n",
    "\n",
    "while len(synthetic_data_list) < num_synthetic_data:\n",
    "    samples = ctgan_synthesizer.sample(num_rows=NUM_ROWS)\n",
    "    synthetic_data_list.extend(samples.loc[samples['ED_2Clases'] == 'H'].values.tolist()[:num_synthetic_data - len(synthetic_data_list)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_df = pd.DataFrame(synthetic_data_list, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|| 6000/6000 [01:47<00:00, 55.96it/s]\n"
     ]
    }
   ],
   "source": [
    "ctgan_synthetic_data_oversampling = ctgan_synthesizer.sample_from_conditions(\n",
    "    conditions=[class_d, class_h],\n",
    "    batch_size = 50,\n",
    "    max_tries_per_batch = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_synthetic_data = pd.concat([df, synthetic_data_df, ctgan_synthetic_data_oversampling], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6496, 77)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctgan_synthetic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgan_synthetic_data.to_csv(\n",
    "    '../../../data/tlp/synthetic-oversampling-ctgan.csv',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. TVAESynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import TVAESynthesizer\n",
    "\n",
    "tvaes_synthesizer = TVAESynthesizer(\n",
    "    metadata,\n",
    "    enforce_min_max_values=True,\n",
    "    enforce_rounding=True,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=True,\n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_synthesizer.auto_assign_transformers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = tvaes_synthesizer.preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -178.370: 100%|| 10000/10000 [07:13<00:00, 23.08it/s]\n"
     ]
    }
   ],
   "source": [
    "tvaes_synthesizer.fit_processed_data(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_list = []\n",
    "\n",
    "while len(synthetic_data_list) < num_synthetic_data:\n",
    "    samples = tvaes_synthesizer.sample(num_rows=NUM_ROWS)\n",
    "    synthetic_data_list.extend(samples.loc[samples['ED_2Clases'] == 'H'].values.tolist()[:num_synthetic_data - len(synthetic_data_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_df = pd.DataFrame(synthetic_data_list, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: 100%|| 6000/6000 [01:54<00:00, 52.38it/s]\n"
     ]
    }
   ],
   "source": [
    "tvaes_synthetic_data_oversampling = tvaes_synthesizer.sample_from_conditions(\n",
    "    conditions=[class_d, class_h],\n",
    "    batch_size = 50,\n",
    "    max_tries_per_batch = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_synthetic_data = pd.concat([df, synthetic_data_df, tvaes_synthetic_data_oversampling], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6496, 77)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvaes_synthetic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvaes_synthetic_data.to_csv(\n",
    "    '../../../data/tlp/synthetic-oversampling-tvaes.csv',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating Real vs. Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import get_column_plot\n",
    "\n",
    "plot_columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: || 77/77 [00:00<00:00, 1820.51it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: || 1/1 [00:00<00:00, 97.82it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "\n",
    "gc_diagnostic = run_diagnostic(\n",
    "    real_data=df,\n",
    "    synthetic_data=gc_synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: || 77/77 [00:00<00:00, 256.66it/s]|\n",
      "Column Shapes Score: 84.45%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: || 2926/2926 [00:11<00:00, 264.00it/s]|\n",
      "Column Pair Trends Score: 85.59%\n",
      "\n",
      "Overall Score (Average): 85.02%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "\n",
    "gc_quality_report = evaluate_quality(\n",
    "    df,\n",
    "    gc_synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc_details = gc_quality_report.get_details('Column Pair Trends')\n",
    "# gc_details[gc_details['Real Correlation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gc_quality_report.get_details('Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in plot_columns:\n",
    "#     fig = get_column_plot(\n",
    "#         real_data=df,\n",
    "#         synthetic_data=gc_synthetic_data,\n",
    "#         column_name=column,\n",
    "#         metadata=metadata\n",
    "#     )\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: || 77/77 [00:00<00:00, 1675.23it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: || 1/1 [00:00<00:00, 254.20it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctgan_diagnostic = run_diagnostic(\n",
    "    real_data=df,\n",
    "    synthetic_data=ctgan_synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: || 77/77 [00:00<00:00, 232.35it/s]|\n",
      "Column Shapes Score: 80.39%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: || 2926/2926 [00:11<00:00, 265.20it/s]|\n",
      "Column Pair Trends Score: 84.38%\n",
      "\n",
      "Overall Score (Average): 82.39%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctgan_quality_report = evaluate_quality(\n",
    "    df,\n",
    "    ctgan_synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctgan_details = ctgan_quality_report.get_details('Column Pair Trends')\n",
    "# ctgan_details[ctgan_details['Real Correlation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctgan_quality_report.get_details('Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for column in plot_columns:\n",
    "#    fig = get_column_plot(\n",
    "#        real_data=df,\n",
    "#        synthetic_data=ctgan_synthetic_data,\n",
    "#        column_name=column,\n",
    "#        metadata=metadata\n",
    "#    )\n",
    "#    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. TVAESSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: || 77/77 [00:00<00:00, 1635.96it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: || 1/1 [00:00<00:00, 724.78it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvaes_diagnostic = run_diagnostic(\n",
    "    real_data=df,\n",
    "    synthetic_data=tvaes_synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: || 77/77 [00:00<00:00, 386.05it/s]|\n",
      "Column Shapes Score: 90.61%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: || 2926/2926 [00:10<00:00, 267.25it/s]|\n",
      "Column Pair Trends Score: 90.41%\n",
      "\n",
      "Overall Score (Average): 90.51%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvaes_quality_report = evaluate_quality(\n",
    "    df,\n",
    "    tvaes_synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvaes_details = tvaes_quality_report.get_details('Column Pair Trends')\n",
    "# tvaes_details[tvaes_details['Real Correlation'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvaes_quality_report.get_details('Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for column in plot_columns:\n",
    "#    fig = get_column_plot(\n",
    "#        real_data=df,\n",
    "#        synthetic_data=tvaes_synthetic_data,\n",
    "#        column_name=column,\n",
    "#        metadata=metadata\n",
    "#    )\n",
    "#    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
