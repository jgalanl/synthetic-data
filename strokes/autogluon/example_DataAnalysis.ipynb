{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-23T16:27:12.330787Z",
     "iopub.status.busy": "2022-11-23T16:27:12.330355Z",
     "iopub.status.idle": "2022-11-23T16:27:12.343736Z",
     "shell.execute_reply": "2022-11-23T16:27:12.342742Z",
     "shell.execute_reply.started": "2022-11-23T16:27:12.33075Z"
    }
   },
   "source": [
    "\n",
    "<div style=\"color:#D81F26;\n",
    "           display:fill;\n",
    "           border-style: solid;\n",
    "           border-color:#C1C1C1;\n",
    "           font-size:14px;\n",
    "           font-family:Calibri;\n",
    "           background-color:#373737;\">\n",
    "<h2 style=\"text-align: center;\n",
    "           padding: 10px;\n",
    "           color:#FFFFFF;\">\n",
    "======= Easy-to-use AutoML (Autogluon, FLAML, AutoSKLearn) =======\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://research.aimultiple.com/wp-content/webp-express/webp-images/uploads/2018/06/good-data-800x400.jpg.webp\" length=700 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML is a valuable tool for data scientists, as it streamlines the process of selecting the optimal machine learning model and hyperparameters while minimizing the need for technical expertise. With AutoML, data scientists can focus on use case application, integration, and model deployment. This notebook utilizes three AutoML packages to compare their performance using the Heart Failure Prediction dataset. AutoML alleviates the overhead of model algorithm comparison and hyperparameter tuning for each algorithm, automatically selecting the best model algorithm for inference.\n",
    "\n",
    "## 1.1. Summary of the Piepline\n",
    "\n",
    "* Check if there are any null values in the features\n",
    "* Handle the outliers for numerical variables by IRQ\n",
    "* Standardize the distribution of numerical variables by RobustScaler\n",
    "* Encode categorical variables by OneHotEncoder\n",
    "* Perform the Autogluon, FLAML and AutoSKLearn automl model and measure performance metrics of Accuracy, Precision, F1, Recall, ROC AUC and Confusion Matrix\n",
    "\n",
    "## 1.2. Observation and Findings\n",
    "\n",
    "The results indicate that <b>Autogluon</b> outperforms the other two packages, while <b>AutoSKLearn</b> performs the worst. One of the reasons for this difference in performance could be the availability of model algorithms. <b>AutoSKLearn</b> only uses models available in SKLearn and forms ensembles from base learners such as regression, decision tree, and kNN. In contrast, <b>FLAML</b> and <b>Autogluon</b> AutoML include advanced model algorithms like XGBoost, LGBM, and CatBoost. Despite the advanced model algorithms, <b>Autogluon</b> AutoML also uses an ensemble classifier based on the base advanced model learners of XGBoost and LGBM. This drives <b>Autogluon</b> to slightly outperform <b>FLAML</b>, as <b>FLAML</b> has no ensemble classifier from the base learners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:27.276543Z",
     "iopub.status.busy": "2023-03-12T16:59:27.275396Z",
     "iopub.status.idle": "2023-03-12T16:59:27.310062Z",
     "shell.execute_reply": "2023-03-12T16:59:27.309083Z",
     "shell.execute_reply.started": "2023-03-12T16:59:27.276406Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:27.313248Z",
     "iopub.status.busy": "2023-03-12T16:59:27.312428Z",
     "iopub.status.idle": "2023-03-12T16:59:28.606287Z",
     "shell.execute_reply": "2023-03-12T16:59:28.6051Z",
     "shell.execute_reply.started": "2023-03-12T16:59:27.313188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Standardization and Encoding\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "\n",
    "# Compariosn for Model Performance\n",
    "from sklearn import model_selection, metrics, naive_bayes\n",
    "\n",
    "# Visualization Library, matplotlib and seaborn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hide convergence warning for now\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Heart Failure Prediction Dataset, which is contributed by @fedesoriano, is used.  The details of the data set can be referred to [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* <div style=\"font-size: 18px\">Age: age of the patient [years]</div>\n",
    "* <div style=\"font-size: 18px\">Sex: sex of the patient [M: Male, F: Female]</div>\n",
    "* <div style=\"font-size: 18px\">ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]</div>\n",
    "* <div style=\"font-size: 18px\">RestingBP: resting blood pressure [mm Hg]</div>\n",
    "* <div style=\"font-size: 18px\">Cholesterol: serum cholesterol [mm/dl]</div>\n",
    "* <div style=\"font-size: 18px\">FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]</div>\n",
    "* <div style=\"font-size: 18px\">RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]</div>\n",
    "* <div style=\"font-size: 18px\">MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]</div>\n",
    "* <div style=\"font-size: 18px\">ExerciseAngina: exercise-induced angina [Y: Yes, N: No]</div>\n",
    "* <div style=\"font-size: 18px\">Oldpeak: oldpeak = ST [Numeric value measured in depression]</div>\n",
    "* <div style=\"font-size: 18px\">ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]</div>\n",
    "* <div style=\"font-size: 18px\">HeartDisease: target class [1: heart disease, 0: Normal]</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:28.608856Z",
     "iopub.status.busy": "2023-03-12T16:59:28.608443Z",
     "iopub.status.idle": "2023-03-12T16:59:28.632302Z",
     "shell.execute_reply": "2023-03-12T16:59:28.631092Z",
     "shell.execute_reply.started": "2023-03-12T16:59:28.608821Z"
    }
   },
   "outputs": [],
   "source": [
    "df_heart = pd.read_csv('../input/heart-failure-prediction/heart.csv')\n",
    "print('No. of row: {}, no. of columns: {}'.format(df_heart.shape[0], df_heart.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Distribution of target feature\n",
    "\n",
    "<div style=\" background-color:#3a5311;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Objective: to check if the distribution of target feature (i.e. heart disease) is balanced.  If it is not the case, we will do oversampling.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:28.634204Z",
     "iopub.status.busy": "2023-03-12T16:59:28.633857Z",
     "iopub.status.idle": "2023-03-12T16:59:28.831779Z",
     "shell.execute_reply": "2023-03-12T16:59:28.830565Z",
     "shell.execute_reply.started": "2023-03-12T16:59:28.634174Z"
    }
   },
   "outputs": [],
   "source": [
    "# check whether the data set is balanced\n",
    "\n",
    "def auto_fmt (pct_value):\n",
    "    return '{:.0f}\\n({:.2f}%)'.format(df_heart['HeartDisease'].value_counts().sum()*pct_value/100,pct_value) \n",
    "\n",
    "df_target_count = df_heart['HeartDisease'].value_counts().rename_axis('HeartDisease').reset_index(name='Counts')\n",
    "\n",
    "# plt.pie(x=df_target_count, labels=df_target_count.index, autopct='%1.2f%%')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6,6)\n",
    "plt.pie(x=df_target_count['Counts'], labels=df_target_count['HeartDisease'], autopct=auto_fmt, textprops={'fontsize': 18})\n",
    "plt.title('Distribution of Target Label (i.e. Heard Disease)',  fontsize = 20)\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#0A1172;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Observation: the distribution of target feature between disgnosed and non-disgnosed cases is quite even.  It is no necessary to do oversampling, e.g. SMOTE.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Missing value replacement\n",
    "<div style=\" background-color:#3a5311;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Objective: to check if the data has missing value.  If it is the case, we will do pre-processing for missing value replacement, e.g. replace null with mode for categorical variables and with mean / median for numerical variables \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:28.835932Z",
     "iopub.status.busy": "2023-03-12T16:59:28.835094Z",
     "iopub.status.idle": "2023-03-12T16:59:28.849711Z",
     "shell.execute_reply": "2023-03-12T16:59:28.848538Z",
     "shell.execute_reply.started": "2023-03-12T16:59:28.835883Z"
    }
   },
   "outputs": [],
   "source": [
    "df_heart.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#0A1172;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Observation: there is no missing value in the data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Outlier analysis and detection\n",
    "\n",
    "<div style=\" background-color:#3a5311;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Objective: Outlier analysis is the process of identifying abnormal or extreme observations in a data set. Since outlier causes distribution to have skew distribution, and decision boundary of a model may be skewed to long-tail side, causing inappropriate decision from models. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:28.854814Z",
     "iopub.status.busy": "2023-03-12T16:59:28.853827Z",
     "iopub.status.idle": "2023-03-12T16:59:28.865459Z",
     "shell.execute_reply": "2023-03-12T16:59:28.864156Z",
     "shell.execute_reply.started": "2023-03-12T16:59:28.854762Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify the type of features into either numerical or categorical by values of the features\n",
    "\n",
    "categorical = [var for var in df_heart.columns if df_heart[var].dtype=='O']\n",
    "numerical = [var for var in df_heart.columns if df_heart[var].dtype != 'O' and (var != 'HeartDisease')]\n",
    "\n",
    "# FastingBS will be automatically identified as numerical feature since it only has value of 0 and 1.  We will manually re-group it into categorical feature\n",
    "\n",
    "categorical.append('FastingBS')\n",
    "numerical.remove('FastingBS')\n",
    "\n",
    "print ('Size of the base {}'.format(df_heart.shape))\n",
    "print ('Total No. of variables {}'.format (len(categorical) + len(numerical)))\n",
    "print ('Total No. of categorical variables {}'.format (len(categorical)))\n",
    "print ('Total No. of numberical variables {}'.format (len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Using Boxplot and Histogram to analyze distribution of numerical variables.  This aims to analyze outliers.  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:28.868021Z",
     "iopub.status.busy": "2023-03-12T16:59:28.867228Z",
     "iopub.status.idle": "2023-03-12T16:59:28.910837Z",
     "shell.execute_reply": "2023-03-12T16:59:28.909673Z",
     "shell.execute_reply.started": "2023-03-12T16:59:28.867974Z"
    }
   },
   "outputs": [],
   "source": [
    "df_heart[numerical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "The Boxplot to analyze outliers</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:28.912773Z",
     "iopub.status.busy": "2023-03-12T16:59:28.912399Z",
     "iopub.status.idle": "2023-03-12T16:59:29.845352Z",
     "shell.execute_reply": "2023-03-12T16:59:29.843824Z",
     "shell.execute_reply.started": "2023-03-12T16:59:28.912742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Histgram for numercial features\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16,16))\n",
    "\n",
    "sns.boxplot(x=\"HeartDisease\",y=\"Age\",data=df_heart, ax=ax[0][0])\n",
    "sns.boxplot(x=\"HeartDisease\",y=\"RestingBP\",data=df_heart, ax=ax[0][1])\n",
    "sns.boxplot(x=\"HeartDisease\",y=\"Cholesterol\",data=df_heart, ax=ax[1][0])\n",
    "sns.boxplot(x=\"HeartDisease\",y=\"MaxHR\",data=df_heart, ax=ax[1][1])\n",
    "sns.boxplot(x=\"HeartDisease\",y=\"Oldpeak\",data=df_heart, ax=ax[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "To use Histogram to analyze skewness of the distribution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:29.847858Z",
     "iopub.status.busy": "2023-03-12T16:59:29.847344Z",
     "iopub.status.idle": "2023-03-12T16:59:31.434254Z",
     "shell.execute_reply": "2023-03-12T16:59:31.432799Z",
     "shell.execute_reply.started": "2023-03-12T16:59:29.847812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Histgram for numercial features\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16,16))\n",
    "\n",
    "for i in range(0, (len(ax.flatten())-1)):\n",
    "#     print('{}, {}'.format(int(i/2),i % 2))\n",
    "    sns.histplot(data=df_heart, x =df_heart[numerical[i]], bins=25, ax=ax[int(i/2),i % 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#0A1172;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Observation: From the histograms, some values of rest BP, cholesterol and old peak are abnormal. We will replace the abnormal values with median of the data features. Also, from the boxplot, there are outliers for some featurs.  In the next step, we are going to remove the outliers. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "    To remove outliers by <b>IRQ method</b> - IQR method is used to identify outliers to set up a boundary outside of Q1 and Q3. Any values that fall outside of the boundary are considered outliers. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:31.436447Z",
     "iopub.status.busy": "2023-03-12T16:59:31.435979Z",
     "iopub.status.idle": "2023-03-12T16:59:31.476419Z",
     "shell.execute_reply": "2023-03-12T16:59:31.47511Z",
     "shell.execute_reply.started": "2023-03-12T16:59:31.436412Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in numerical:\n",
    "    if col != 'FastingBS': # Skip this variable since it is skewed\n",
    "        p75 = df_heart[df_heart[col] > 0][col].quantile(0.75)\n",
    "        p25 = df_heart[df_heart[col] > 0][col].quantile(0.25)\n",
    "        iqr = p75 - p25\n",
    "        upper_limit = p75 + (1.5 * iqr)\n",
    "        print('===={} with Upper Limit {:6.1f}, P75 {:6.1f}, P25 {:6.1f}, {} Outlier Records ========'.format(col, upper_limit, p75, p25, df_heart[df_heart[col] > upper_limit]['HeartDisease'].count()))\n",
    "        df_heart[col] = np.where (df_heart[col] > upper_limit, upper_limit, df_heart[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "To replace abnormal Cholesterol and Oldpeak with Median of the features </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:31.482791Z",
     "iopub.status.busy": "2023-03-12T16:59:31.481919Z",
     "iopub.status.idle": "2023-03-12T16:59:31.495127Z",
     "shell.execute_reply": "2023-03-12T16:59:31.493926Z",
     "shell.execute_reply.started": "2023-03-12T16:59:31.482728Z"
    }
   },
   "outputs": [],
   "source": [
    "# For Cholesterol and Resting BP, we replace abnormal values with the median.\n",
    "median_Cholesterol = df_heart['Cholesterol'].median()\n",
    "median_RestingBP = df_heart['RestingBP'].median()\n",
    "\n",
    "# For Old Peak, we replace abnormal values with the 0.\n",
    "# median_Oldpeak = df_heart['Oldpeak'].median()\n",
    "\n",
    "df_heart['Cholesterol'] = np.where(df_heart['Cholesterol'] <= 70, median_Cholesterol, df_heart['Cholesterol'])\n",
    "df_heart['RestingBP'] = np.where(df_heart['RestingBP'] <= 50, median_RestingBP, df_heart['RestingBP'])\n",
    "df_heart['Oldpeak'] = np.where(df_heart['Oldpeak'] < 0, 0, df_heart['Oldpeak'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "To update the Histogram to check for abnormal and outlier</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:31.497255Z",
     "iopub.status.busy": "2023-03-12T16:59:31.496777Z",
     "iopub.status.idle": "2023-03-12T16:59:33.06935Z",
     "shell.execute_reply": "2023-03-12T16:59:33.067857Z",
     "shell.execute_reply.started": "2023-03-12T16:59:31.497206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Updated histogram after the outlier replacement\n",
    "# Histgram for numercial features\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16,16))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    sns.histplot(data=df_heart, x =df_heart[numerical[i]], bins=25, ax=ax[int(i/2),i % 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#0A1172;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Observation: the IRQ is used to remove outliers.  In addition, from the histograms, some values of rest BP, cholesterol and old peak are abnormal. We will replace the abnormal values. \n",
    "<br><br>\n",
    "After the replacement of missing values and handling of abnormal values, the histogram looks more rational. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Standardization and Encoding\n",
    "<div style=\" background-color:#3a5311;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Objective: Data standardization is to pull all numerical variables into the same scale so that the discriminative power of features with high ranged value cannot dominate to the importance of target  due to high absolute value of features.  Data encoding is a process to convert categorical features into numerical values so that the distance between values in categorical features can be measured. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "To standardize the numerical variables with the use of Robust Scaler.  The choice of Robust Scaler over Standard Scaler is  the capability to handle outliers. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:33.071778Z",
     "iopub.status.busy": "2023-03-12T16:59:33.071234Z",
     "iopub.status.idle": "2023-03-12T16:59:33.09091Z",
     "shell.execute_reply": "2023-03-12T16:59:33.089478Z",
     "shell.execute_reply.started": "2023-03-12T16:59:33.071731Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "encoder_num = scaler.fit_transform(df_heart[numerical])\n",
    "encoded_num = pd.DataFrame(encoder_num, columns =numerical)\n",
    "encoded_num.shape\n",
    "print(encoded_num.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "To visualize the distribution of feature values by categorical features </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:33.093461Z",
     "iopub.status.busy": "2023-03-12T16:59:33.092942Z",
     "iopub.status.idle": "2023-03-12T16:59:34.017982Z",
     "shell.execute_reply": "2023-03-12T16:59:34.016577Z",
     "shell.execute_reply.started": "2023-03-12T16:59:33.093415Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots (3, 2, figsize=(16, 16))\n",
    "ax_rst = []\n",
    "\n",
    "for i in range(len(categorical)):\n",
    "    axs = sns.countplot(data=df_heart, x =df_heart[categorical[i]], ax=ax[int(i/2),i % 2])\n",
    "    ax_rst.append(axs)\n",
    "    total = df_heart[categorical[i]].value_counts().sum()\n",
    "    for p in axs.patches:\n",
    "        value_pct = '{:.0f} ({:.1f}%)'.format(p.get_height(), 100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width()/2\n",
    "        y = p.get_height()\n",
    "        axs.annotate(value_pct, (x, y),ha='center')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "One-hot Encoding is used to convert categorical features into binary vectors.  Each unique category in the original data has been transformed into a binary vector, where each element of the vector represents whether or not that category is present in the original data for that particular row.\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.02039Z",
     "iopub.status.busy": "2023-03-12T16:59:34.02Z",
     "iopub.status.idle": "2023-03-12T16:59:34.046649Z",
     "shell.execute_reply": "2023-03-12T16:59:34.045432Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.020356Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the One-hot Encoder \n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# print(categorical)\n",
    "\n",
    "#for col in categorical:\n",
    "encoder_cat = encoder.fit_transform(df_heart[categorical])\n",
    "# encoded_cat = pd.DataFrame(encoder_cat.toarray(), columns=['Sex_F', 'Sex_M', 'ChestPainType_ASY', 'ChestPainType_ATA',\n",
    "#        'ChestPainType_NAP', 'ChestPainType_TA', 'RestingECG_LVH', 'RestingECG_Normal', 'RestingECG_ST', 'ExerciseAngina_N',\n",
    "#        'ExerciseAngina_Y', 'ST_Slope_Down', 'ST_Slope_Flat', 'ST_Slope_Up', 'FastingBS_0', 'FastingBS_1'])\n",
    "encoded_cat = pd.DataFrame(encoder_cat.toarray(), columns=encoder.get_feature_names_out().tolist())\n",
    "\n",
    "\n",
    "print(encoded_cat.head(5))\n",
    "\n",
    "# encoded_cat.columns = encoded_cat.columns.str.replace('[#,@,&]()', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#0A1172;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "Observation: With data encoding, the categorical features are converted into binary vectors which can be used as inputs to most modeling algorithms. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "The first step is to set up the training data set.  Next, the training data set is split into training and testing data sets for modelling.  Different modelling algorithms are used for performance comparison.  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.0487Z",
     "iopub.status.busy": "2023-03-12T16:59:34.048291Z",
     "iopub.status.idle": "2023-03-12T16:59:34.069511Z",
     "shell.execute_reply": "2023-03-12T16:59:34.067888Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.048664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the training data set\n",
    "df_train = df_heart.copy()\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Preparation of Modelling data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "This step is to set up the data set for modelling based on the standardized and encoded features in the previous steps.  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.071884Z",
     "iopub.status.busy": "2023-03-12T16:59:34.071322Z",
     "iopub.status.idle": "2023-03-12T16:59:34.121839Z",
     "shell.execute_reply": "2023-03-12T16:59:34.120289Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.071822Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.drop(numerical, axis=1, inplace=True)\n",
    "df_train.drop(categorical, axis=1, inplace=True)\n",
    "df_train = pd.concat([df_train, encoded_num, encoded_cat], axis=1) \n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "To prepare the training and testing data sets.   </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.123784Z",
     "iopub.status.busy": "2023-03-12T16:59:34.123283Z",
     "iopub.status.idle": "2023-03-12T16:59:34.130941Z",
     "shell.execute_reply": "2023-03-12T16:59:34.129644Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.123749Z"
    }
   },
   "outputs": [],
   "source": [
    "# The first column is the target label\n",
    "y = df_train.iloc[:,0:1]\n",
    "\n",
    "# The rest of columns are features\n",
    "X = df_train.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.132809Z",
     "iopub.status.busy": "2023-03-12T16:59:34.132356Z",
     "iopub.status.idle": "2023-03-12T16:59:34.145632Z",
     "shell.execute_reply": "2023-03-12T16:59:34.144741Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.132765Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "print('size of X_train {} and y_train {}'.format (X_train.shape, y_train.shape))\n",
    "print('size of X_test {} and y_test {}'.format (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.14782Z",
     "iopub.status.busy": "2023-03-12T16:59:34.147065Z",
     "iopub.status.idle": "2023-03-12T16:59:34.196505Z",
     "shell.execute_reply": "2023-03-12T16:59:34.195577Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.147781Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\" background-color:#d0312d;text-align:left; padding: 13px 13px; border-radius: 8px; color: white; font-size: 16px\">\n",
    "This function is used to log model performance for comparison. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.198696Z",
     "iopub.status.busy": "2023-03-12T16:59:34.198002Z",
     "iopub.status.idle": "2023-03-12T16:59:34.212903Z",
     "shell.execute_reply": "2023-03-12T16:59:34.211624Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.198659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe for performance comparison\n",
    "df_performance = pd.DataFrame(columns=['Model', 'Balanced Accuracy', 'Accuracy', 'Precision', 'F1', 'Recall', 'ROC AUC'])\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.21612Z",
     "iopub.status.busy": "2023-03-12T16:59:34.214891Z",
     "iopub.status.idle": "2023-03-12T16:59:34.242311Z",
     "shell.execute_reply": "2023-03-12T16:59:34.240892Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.216068Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_performance (p_test, p_train, p_test_prob, p_train_prob, Y_test, y_train, model_name):\n",
    "    global df_performance\n",
    "    predicted_test = pd.DataFrame(p_test)\n",
    "    predicted_train = pd.DataFrame(p_train)\n",
    "    print('=============================================')\n",
    "    print('Scoring Metrics for {} (Validation)'.format(model_name))\n",
    "    print('=============================================')\n",
    "    print('Balanced Accuracy Score = {:2.3f}'.format(metrics.balanced_accuracy_score(Y_test, predicted_test)))\n",
    "    print('Accuracy Score = {:2.3f}'.format(metrics.accuracy_score(Y_test, predicted_test)))\n",
    "    print('Precision Score = {:2.3f}'.format(metrics.precision_score(Y_test, predicted_test)))\n",
    "    print('F1 Score = {:2.3f}'.format(metrics.f1_score(Y_test, predicted_test, labels=['0','1'])))\n",
    "    print('Recall Score = {:2.3f}'.format(metrics.recall_score(Y_test, predicted_test, labels=['0','1'])))\n",
    "    print('ROC AUC Score = {:2.3f}'.format(metrics.roc_auc_score(Y_test, predicted_test, labels=['0','1'])))\n",
    "    print('Confusion Matrix')\n",
    "    print('==================')\n",
    "    print(metrics.confusion_matrix(Y_test, predicted_test))\n",
    "    print('==================')\n",
    "    print(metrics.classification_report(Y_test, predicted_test, target_names=['0','1']))\n",
    "    metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(Y_test, predicted_test)).plot()\n",
    "\n",
    "    df_performance = df_performance.append({'Model':model_name\n",
    "                                            , 'Balanced Accuracy': metrics.balanced_accuracy_score(Y_test, predicted_test)\n",
    "                                            , 'Accuracy' :metrics.accuracy_score(Y_test, predicted_test)\n",
    "                                            , 'Precision' :metrics.precision_score(Y_test, predicted_test)\n",
    "                                            , 'F1':metrics.f1_score(Y_test, predicted_test, labels=['0','1'])\n",
    "                                            , 'Recall': metrics.recall_score(Y_test, predicted_test, labels=['0','1'])\n",
    "                                            , 'ROC AUC': metrics.roc_auc_score(Y_test, predicted_test, labels=['0','1'])\n",
    "                                           }, ignore_index = True)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr_test, tpr_test, _ = metrics.roc_curve(Y_test, p_test_prob)\n",
    "\n",
    "    roc_auc_test = metrics.roc_auc_score(Y_test, predicted_test, labels=['0','1'])\n",
    "\n",
    "    # Precision x Recall Curve\n",
    "    precision_test, recall_test, thresholds_test = metrics.precision_recall_curve(Y_test, p_test_prob)\n",
    "\n",
    "    print('=============================================')\n",
    "    print('Scoring Metrics for {} (Training)'.format(model_name))\n",
    "    print('=============================================')\n",
    "    print('Balanced Accuracy Score = {:2.3f}'.format(metrics.balanced_accuracy_score(y_train, predicted_train)))\n",
    "    print('Accuracy Score = {:2.3f}'.format(metrics.accuracy_score(y_train, predicted_train)))\n",
    "    print('Precision Score = {:2.3f}'.format(metrics.precision_score(y_train, predicted_train)))\n",
    "    print('F1 Score = {:2.3f}'.format(metrics.f1_score(y_train, predicted_train)))\n",
    "    print('Recall Score = {:2.3f}'.format(metrics.recall_score(y_train, predicted_train, labels=['0','1'])))\n",
    "    print('ROC AUC Score = {:2.3f}'.format(metrics.roc_auc_score(y_train, predicted_train, labels=['0','1'])))\n",
    "    print('Confusion Matrix')\n",
    "    print('==================')\n",
    "    print(metrics.confusion_matrix(y_train, predicted_train))\n",
    "    print('==================')\n",
    "    print(metrics.classification_report(y_train, predicted_train, target_names=['0','1']))\n",
    "    metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(y_train, predicted_train)).plot()\n",
    "\n",
    "    fpr_train, tpr_train, _ = metrics.roc_curve(y_train, p_train_prob)\n",
    "\n",
    "    roc_auc_train = metrics.roc_auc_score(y_train, predicted_train, labels=['0','1'])\n",
    "\n",
    "\n",
    "    print('======= ROC Curve =======')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 5))    \n",
    "    plt.plot(fpr_test, tpr_test, color='darkorange', label='ROC curve - Validation (area = %0.3f)' % roc_auc_test)\n",
    "    plt.plot(fpr_train, tpr_train, color='darkblue', label='ROC curve - Training (area = %0.3f)' % roc_auc_train)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. AutoML - AutoSKLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Install the AutoSKLearn Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T16:59:34.244892Z",
     "iopub.status.busy": "2023-03-12T16:59:34.243957Z",
     "iopub.status.idle": "2023-03-12T17:01:24.400623Z",
     "shell.execute_reply": "2023-03-12T17:01:24.398953Z",
     "shell.execute_reply.started": "2023-03-12T16:59:34.244839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need to intall the latest sklearn in order to use the auto-sklearn\n",
    "!apt-get remove swig\n",
    "!apt-get install swig3.0 build-essential -y\n",
    "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "!apt-get install build-essential\n",
    "!pip install --upgrade setuptools\n",
    "!pip install auto-sklearn\n",
    "!pip install -U scikit-learn\n",
    "!pip freeze | grep scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T17:01:24.402967Z",
     "iopub.status.busy": "2023-03-12T17:01:24.402539Z",
     "iopub.status.idle": "2023-03-12T17:04:21.780358Z",
     "shell.execute_reply": "2023-03-12T17:04:21.779048Z",
     "shell.execute_reply.started": "2023-03-12T17:01:24.402926Z"
    }
   },
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "\n",
    "SK_automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "#    time_left_for_this_task=1800,\n",
    "    time_left_for_this_task=180,    \n",
    "    per_run_time_limit=40,\n",
    "    resampling_strategy='cv',\n",
    "    resampling_strategy_arguments={'folds': 5}\n",
    ")\n",
    "SK_automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:04:21.782811Z",
     "iopub.status.busy": "2023-03-12T17:04:21.78225Z",
     "iopub.status.idle": "2023-03-12T17:04:21.788689Z",
     "shell.execute_reply": "2023-03-12T17:04:21.787466Z",
     "shell.execute_reply.started": "2023-03-12T17:04:21.782758Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(np.argmax(SK_automl.cv_results_['mean_test_score']))\n",
    "# print(SK_automl.cv_results_['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:04:21.791335Z",
     "iopub.status.busy": "2023-03-12T17:04:21.790257Z",
     "iopub.status.idle": "2023-03-12T17:04:33.767933Z",
     "shell.execute_reply": "2023-03-12T17:04:33.765988Z",
     "shell.execute_reply.started": "2023-03-12T17:04:21.791297Z"
    }
   },
   "outputs": [],
   "source": [
    "p_train_AutoSKLearn = SK_automl.predict(X_train)\n",
    "p_test_AutoSKLearn = SK_automl.predict(X_test)\n",
    "p_train_proba_AutoSKLearn = SK_automl.predict_proba(X_train)[:,1]\n",
    "p_test_proba_AutoSKLearn = SK_automl.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:04:33.771045Z",
     "iopub.status.busy": "2023-03-12T17:04:33.770127Z",
     "iopub.status.idle": "2023-03-12T17:04:34.625965Z",
     "shell.execute_reply": "2023-03-12T17:04:34.624672Z",
     "shell.execute_reply.started": "2023-03-12T17:04:33.770996Z"
    }
   },
   "outputs": [],
   "source": [
    "model_performance(p_test_AutoSKLearn, p_train_AutoSKLearn, p_test_proba_AutoSKLearn, p_train_proba_AutoSKLearn, y_test, y_train, 'AutoSKLearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. AutoML - Autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Install the Autogluon Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T17:04:34.628913Z",
     "iopub.status.busy": "2023-03-12T17:04:34.627771Z",
     "iopub.status.idle": "2023-03-12T17:05:34.106473Z",
     "shell.execute_reply": "2023-03-12T17:05:34.105413Z",
     "shell.execute_reply.started": "2023-03-12T17:04:34.628869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this if autogluon is not already installed\n",
    "# !pip install \"mxnet<2.0.0\"\n",
    "# !pip install autogluon\n",
    "\n",
    "!pip install autogluon\n",
    "import os,warnings,sys\n",
    "import numpy as np\n",
    "import torch as tc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T17:05:34.114372Z",
     "iopub.status.busy": "2023-03-12T17:05:34.112188Z",
     "iopub.status.idle": "2023-03-12T17:05:34.163177Z",
     "shell.execute_reply": "2023-03-12T17:05:34.161677Z",
     "shell.execute_reply.started": "2023-03-12T17:05:34.114304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training models\n",
    "X_autogluon = pd.concat([X_train, y_train], axis=1)\n",
    "X_autogluon.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T17:05:34.165418Z",
     "iopub.status.busy": "2023-03-12T17:05:34.164902Z",
     "iopub.status.idle": "2023-03-12T17:07:24.820439Z",
     "shell.execute_reply": "2023-03-12T17:07:24.819042Z",
     "shell.execute_reply.started": "2023-03-12T17:05:34.165369Z"
    }
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "Gluon_automl = TabularPredictor(label='HeartDisease', eval_metric='roc_auc').fit(X_autogluon, presets='best_quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:07:24.824499Z",
     "iopub.status.busy": "2023-03-12T17:07:24.822897Z",
     "iopub.status.idle": "2023-03-12T17:07:27.446349Z",
     "shell.execute_reply": "2023-03-12T17:07:27.444917Z",
     "shell.execute_reply.started": "2023-03-12T17:07:24.824436Z"
    }
   },
   "outputs": [],
   "source": [
    "p_train_AutoGluon = Gluon_automl.predict(X_train)\n",
    "p_test_AutoGluon = Gluon_automl.predict(X_test)\n",
    "p_train_proba_AutoGluon = Gluon_automl.predict_proba(X_train).iloc[:,1]\n",
    "p_test_proba_AutoGluon = Gluon_automl.predict_proba(X_test).iloc[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:07:27.44981Z",
     "iopub.status.busy": "2023-03-12T17:07:27.448352Z",
     "iopub.status.idle": "2023-03-12T17:07:28.215261Z",
     "shell.execute_reply": "2023-03-12T17:07:28.213485Z",
     "shell.execute_reply.started": "2023-03-12T17:07:27.449753Z"
    }
   },
   "outputs": [],
   "source": [
    "model_performance(p_test_AutoGluon, p_train_AutoGluon, p_test_proba_AutoGluon, p_train_proba_AutoGluon, y_test, y_train, 'AutoGluon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. AutoML - FLAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Install the FLAML Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T17:07:28.217814Z",
     "iopub.status.busy": "2023-03-12T17:07:28.216977Z",
     "iopub.status.idle": "2023-03-12T17:07:42.863131Z",
     "shell.execute_reply": "2023-03-12T17:07:42.861895Z",
     "shell.execute_reply.started": "2023-03-12T17:07:28.217768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install  the FLAML package\n",
    "!pip install -q flaml\n",
    "from flaml import AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-03-12T17:07:42.865668Z",
     "iopub.status.busy": "2023-03-12T17:07:42.865281Z",
     "iopub.status.idle": "2023-03-12T17:10:42.711309Z",
     "shell.execute_reply": "2023-03-12T17:10:42.709852Z",
     "shell.execute_reply.started": "2023-03-12T17:07:42.865633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an AutoML instance\n",
    "FL_automl = AutoML()\n",
    "\n",
    "# Specify automl goal and constraint\n",
    "FL_automl_settings = {\n",
    "    # in seconds    \n",
    "#    \"time_budget\": 1800, \n",
    "    \"time_budget\": 180,  \n",
    "    \"metric\": 'accuracy',\n",
    "    \"task\": 'classification',\n",
    "    \"verbose\": 0,\n",
    "    \"n_jobs\": -1,\n",
    "    \"eval_method\": 'cv',\n",
    "    \"n_splits\":5\n",
    "}\n",
    "\n",
    "# Train with labeled input data\n",
    "FL_automl.fit(X_train, y_train.to_numpy(), **FL_automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:10:42.713557Z",
     "iopub.status.busy": "2023-03-12T17:10:42.713032Z",
     "iopub.status.idle": "2023-03-12T17:10:42.722879Z",
     "shell.execute_reply": "2023-03-12T17:10:42.721219Z",
     "shell.execute_reply.started": "2023-03-12T17:10:42.713433Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Best ML leaner:', FL_automl.best_estimator)\n",
    "print('Best hyperparmeter config:', FL_automl.best_config)\n",
    "print('Training duration of best run: {0:.4g} s', FL_automl.best_config_train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:10:42.725342Z",
     "iopub.status.busy": "2023-03-12T17:10:42.724909Z",
     "iopub.status.idle": "2023-03-12T17:10:42.778287Z",
     "shell.execute_reply": "2023-03-12T17:10:42.777103Z",
     "shell.execute_reply.started": "2023-03-12T17:10:42.725305Z"
    }
   },
   "outputs": [],
   "source": [
    "p_train_FLAML = FL_automl.predict(X_train)\n",
    "p_test_FLAML = FL_automl.predict(X_test)\n",
    "p_train_proba_FLAML = FL_automl.predict_proba(X_train)[:,1]\n",
    "p_test_proba_FLAML = FL_automl.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:10:42.780558Z",
     "iopub.status.busy": "2023-03-12T17:10:42.780097Z",
     "iopub.status.idle": "2023-03-12T17:10:43.998721Z",
     "shell.execute_reply": "2023-03-12T17:10:43.996294Z",
     "shell.execute_reply.started": "2023-03-12T17:10:42.780521Z"
    }
   },
   "outputs": [],
   "source": [
    "model_performance(p_test_FLAML, p_train_FLAML, p_test_proba_FLAML, p_train_proba_FLAML, y_test, y_train, 'FLAML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Performance Comparison and Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Performance Comparison for AutoML Packgaes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:10:44.000727Z",
     "iopub.status.busy": "2023-03-12T17:10:44.000293Z",
     "iopub.status.idle": "2023-03-12T17:10:44.023977Z",
     "shell.execute_reply": "2023-03-12T17:10:44.022601Z",
     "shell.execute_reply.started": "2023-03-12T17:10:44.00069Z"
    }
   },
   "outputs": [],
   "source": [
    "df_performance = df_performance.round(2)\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:10:44.026273Z",
     "iopub.status.busy": "2023-03-12T17:10:44.025758Z",
     "iopub.status.idle": "2023-03-12T17:10:45.069936Z",
     "shell.execute_reply": "2023-03-12T17:10:45.068544Z",
     "shell.execute_reply.started": "2023-03-12T17:10:44.026224Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_review = df_performance['Model'].values.tolist()\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15,14), squeeze=False)\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.barplot(x = cols_review, y = df_performance['Balanced Accuracy'], alpha=0.9,ax = axs[0][0])\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax = sns.barplot(x = cols_review, y = df_performance['Accuracy'], alpha=0.9,ax = axs[0][1])\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax = sns.barplot(x = cols_review, y = df_performance['Precision'], alpha=0.9,ax = axs[1][0])\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax = sns.barplot(x = cols_review, y = df_performance['F1'], alpha=0.9,ax = axs[1][1])\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax = sns.barplot(x = cols_review, y = df_performance['Recall'], alpha=0.9,ax = axs[2][0])\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax = sns.barplot(x = cols_review, y = df_performance['ROC AUC'], alpha=0.9,ax = axs[2][1])\n",
    "ax.bar_label(ax.containers[0])\n",
    "\n",
    "plt.tight_layout()    \n",
    "fig.autofmt_xdate(rotation=90)\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left; padding: 13px 13px; border-radius: 8px; font-size: 16px\">\n",
    "The results indicate that <b>Autogluon</b> outperforms the other two packages, while <b>AutoSKLearn</b> performs the worst. One of the reasons for this difference in performance could be the availability of model algorithms. <b>AutoSKLearn</b> only uses models available in SKLearn and forms ensembles from base learners such as regression, decision tree, and kNN. In contrast, <b>FLAML</b> and <b>Autogluon</b> AutoML include advanced model algorithms like XGBoost, LGBM, and CatBoost. Despite the advanced model algorithms, <b>Autogluon</b> AutoML also uses an ensemble classifier based on the base advanced model learners of XGBoost and LGBM. This drives <b>Autogluon</b> to slightly outperform <b>FLAML</b>, as <b>FLAML</b> has no ensemble classifier from the base learners.\n",
    "\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for your time to read my notebook.  If you like my work, please give me an \"upvote\" as appreciation.  Happy Kaggling together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1582403,
     "sourceId": 2603715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
