{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import TVAESynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 50\n",
    "NUM_EPOCHS = 5_000\n",
    "NUM_SYNT_DATA = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar customizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('distributions.json') as f:\n",
    "    distributions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('constraints.json') as f:\n",
    "    constraints = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 140.36it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 143.89it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 146.40it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 147.64it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 146.02it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 139.93it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 145.68it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 146.26it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 144.07it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling conditions: : 350it [00:02, 145.78it/s]                       \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    numerical_columns = [\n",
    "        'demo-genero',\n",
    "        'clin-reservaCognitiva_escolaridad'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=numerical_columns,\n",
    "        sdtype='numerical',\n",
    "        computer_representation=\"Int8\",\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "    if os.path.exists('gc_metadata.json'):\n",
    "        os.remove('gc_metadata.json')\n",
    "    metadata.save_to_json('gc_metadata.json')\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    # num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=num_h,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        # num_rows=num_h + num_synthetic_data,\n",
    "        num_rows=num_d,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    gc_synthesizer = GaussianCopulaSynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=False,\n",
    "        enforce_rounding=True,\n",
    "        # numerical_distributions=distributions,\n",
    "        default_distribution='gaussian_kde'\n",
    "    )\n",
    "\n",
    "    gc_synthesizer.add_constraints(constraints)\n",
    "    gc_synthesizer.fit(df)\n",
    "\n",
    "    gc_synthetic_data = gc_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "\n",
    "    # gc_synthetic_data = pd.concat([df, gc_synthetic_data], ignore_index=True)\n",
    "    gc_synthetic_data.to_csv(\n",
    "        f'../data/synthetic/gc/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-0.55) | Discrim. (-0.18): 100%|██████████| 5000/5000 [04:48<00:00, 17.36it/s]\n",
      "Sampling conditions: : 400it [00:03, 117.49it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-0.98) | Discrim. (0.28): 100%|██████████| 5000/5000 [04:47<00:00, 17.41it/s] \n",
      "Sampling conditions: : 400it [00:03, 114.39it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-1.48) | Discrim. (-0.17): 100%|██████████| 5000/5000 [04:50<00:00, 17.20it/s]\n",
      "Sampling conditions: : 400it [00:03, 115.38it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-0.56) | Discrim. (-0.21): 100%|██████████| 5000/5000 [04:47<00:00, 17.41it/s]\n",
      "Sampling conditions: : 400it [00:03, 116.47it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-1.40) | Discrim. (0.08): 100%|██████████| 5000/5000 [04:49<00:00, 17.28it/s] \n",
      "Sampling conditions: : 400it [00:03, 116.87it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-1.45) | Discrim. (-0.10): 100%|██████████| 5000/5000 [04:40<00:00, 17.82it/s]\n",
      "Sampling conditions: : 400it [00:03, 116.42it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-1.04) | Discrim. (-0.05): 100%|██████████| 5000/5000 [04:48<00:00, 17.32it/s]\n",
      "Sampling conditions: : 400it [00:03, 116.35it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-1.41) | Discrim. (-0.03): 100%|██████████| 5000/5000 [04:46<00:00, 17.46it/s]\n",
      "Sampling conditions: : 400it [00:03, 115.29it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-0.85) | Discrim. (-0.02): 100%|██████████| 5000/5000 [04:50<00:00, 17.18it/s]\n",
      "Sampling conditions: : 400it [00:03, 116.61it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-0.78) | Discrim. (-0.21): 100%|██████████| 5000/5000 [04:50<00:00, 17.21it/s]\n",
      "Sampling conditions: : 400it [00:03, 116.50it/s]                       \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    numerical_columns = [\n",
    "        'demo-genero',\n",
    "        'clin-reservaCognitiva_escolaridad'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=numerical_columns,\n",
    "        sdtype='numerical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "    if os.path.exists('ctgan_metadata.json'):\n",
    "        os.remove('ctgan_metadata.json')\n",
    "    metadata.save_to_json('ctgan_metadata.json')\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    ctgan_synthesizer = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=False,\n",
    "        enforce_rounding=True,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        cuda=True\n",
    "    )\n",
    "\n",
    "    ctgan_synthesizer.add_constraints(constraints)\n",
    "    ctgan_synthesizer.fit(df)\n",
    "\n",
    "    ctgan_synthetic_data = ctgan_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    ctgan_synthetic_data = pd.concat([df, ctgan_synthetic_data], ignore_index=True)\n",
    "    ctgan_synthetic_data.to_csv(\n",
    "        f'../data/synthetic/ctgan/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TVAESynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -107.670: 100%|██████████| 5000/5000 [01:46<00:00, 47.01it/s]\n",
      "Sampling conditions: : 400it [00:03, 118.84it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -113.139: 100%|██████████| 5000/5000 [01:42<00:00, 48.57it/s]\n",
      "Sampling conditions: : 400it [00:03, 127.69it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -113.680: 100%|██████████| 5000/5000 [01:44<00:00, 47.62it/s]\n",
      "Sampling conditions: : 400it [00:02, 142.17it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -116.593: 100%|██████████| 5000/5000 [01:45<00:00, 47.61it/s]\n",
      "Sampling conditions: : 400it [00:03, 122.44it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -110.048: 100%|██████████| 5000/5000 [01:44<00:00, 48.04it/s]\n",
      "Sampling conditions: : 400it [00:02, 147.56it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -119.541: 100%|██████████| 5000/5000 [01:44<00:00, 47.75it/s]\n",
      "Sampling conditions: : 400it [00:03, 119.51it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -115.175: 100%|██████████| 5000/5000 [01:45<00:00, 47.53it/s]\n",
      "Sampling conditions: : 400it [00:03, 107.39it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -113.962: 100%|██████████| 5000/5000 [01:44<00:00, 47.94it/s]\n",
      "Sampling conditions: : 400it [00:03, 115.60it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -110.293: 100%|██████████| 5000/5000 [01:44<00:00, 47.77it/s]\n",
      "Sampling conditions: : 400it [00:02, 138.61it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for set 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -113.074: 100%|██████████| 5000/5000 [01:46<00:00, 46.83it/s]\n",
      "Sampling conditions: : 400it [00:03, 115.93it/s]                       \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    numerical_columns = [\n",
    "        'demo-genero',\n",
    "        'clin-reservaCognitiva_escolaridad'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=numerical_columns,\n",
    "        sdtype='numerical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "    if os.path.exists('tvaes_metadata.json'):\n",
    "        os.remove('tvaes_metadata.json')\n",
    "    metadata.save_to_json('tvaes_metadata.json')\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    tvaes_synthesizer = TVAESynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=False,\n",
    "        enforce_rounding=True,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        cuda=True\n",
    "    )\n",
    "\n",
    "    tvaes_synthesizer.add_constraints(constraints)\n",
    "    tvaes_synthesizer.fit(df)\n",
    "    tvaes_synthetic_data = tvaes_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    tvaes_synthetic_data = pd.concat([df, tvaes_synthetic_data], ignore_index=True)\n",
    "    tvaes_synthetic_data.to_csv(\n",
    "        f'../data/synthetic/tvaes/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CopulaGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'Generating synthetic data for set {i}')\n",
    "    df = pd.read_csv(f'../../data/train/set_{i}.csv')\n",
    "    metadata = Metadata()\n",
    "    metadata.detect_table_from_dataframe(data=df, table_name='TLP')\n",
    "    categorical_columns = [\n",
    "        'ED_2Clases', 'clin-frecUsoEmail'\n",
    "    ]\n",
    "    metadata.update_columns(\n",
    "        column_names=categorical_columns,\n",
    "        sdtype='categorical',\n",
    "        table_name='TLP'\n",
    "    )\n",
    "    metadata.validate()\n",
    "\n",
    "    num_d, num_h = df['ED_2Clases'].value_counts()\n",
    "    num_synthetic_data = num_d - num_h\n",
    "\n",
    "    class_d = Condition(\n",
    "        num_rows=NUM_SYNT_DATA,\n",
    "        column_values={'ED_2Clases': 'D'}\n",
    "    )\n",
    "\n",
    "    class_h = Condition(\n",
    "        num_rows=NUM_SYNT_DATA + num_synthetic_data,\n",
    "        column_values={'ED_2Clases': 'H'}\n",
    "    )\n",
    "\n",
    "    cg_synthesizer = CopulaGANSynthesizer(\n",
    "        metadata,\n",
    "        enforce_min_max_values=True,\n",
    "        enforce_rounding=True,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        cuda=True\n",
    "    )\n",
    "\n",
    "    cg_synthesizer.auto_assign_transformers(df)\n",
    "    processed_df = cg_synthesizer.preprocess(df)\n",
    "    cg_synthesizer.fit_processed_data(processed_df)\n",
    "    cg_synthetic_data = cg_synthesizer.sample_from_conditions(\n",
    "        conditions=[class_d, class_h],\n",
    "        batch_size = 50,\n",
    "        max_tries_per_batch = 100\n",
    "    )\n",
    "    cg_synthetic_data = pd.concat([df, cg_synthetic_data], ignore_index=True)\n",
    "    cg_synthetic_data.to_csv(\n",
    "        f'../../data/synthetic/cg/set_{i}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
